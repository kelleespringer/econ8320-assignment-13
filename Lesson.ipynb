{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0diJuC1uq5Q"
      },
      "source": [
        "# Statistical Modeling in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "yPnYIYi3uq5S"
      },
      "source": [
        "### Customization vs Rapid Development\n",
        "\n",
        "As we know from (painful?) experience, Python is powerful because of its ability to leverage `numpy` and `scipy` to implement any statistical model from scratch. We can write the requisite matrix algebra, or the relevant likelihood function, and from there can optimize our model, calculate confidence intervals, and report the output of that model through data frames, lists, or printed tables. Building our own models is great! We get to build a model based on the exact context and assumptions of our problem, and therefore get exactly the model that we wanted. Unfortunately, it takes a LOT of time!\n",
        "\n",
        "This lesson will provide our first exposure to pre-written statistical modeling in Python. We will be able to use only a couple of lines of code to implement complex and valuable statistical and machine learning models. Because the most costly asset in programming is the time that we spend debugging and writing code (running code is MUCH faster and cheaper than the time spent writing code), we are always looking for ways to avoid writing code that someone else has already written.\n",
        "\n",
        "`statsmodels` is a library that covers the majority of regression models commonly used by economists and statisticians in other fields.\n",
        "\n",
        "`sklearn` is an analogous library that covers machine learning models (aside from deep neural networks, which have their own implementations).\n",
        "\n",
        "Each of these libraries is highly optimized to provide performant implementations of models that we use regularly, and allow us to avoid writing these models from scratch unless we need to customize our model for some specific use case! This is great news! You'll never have to think about writing your own linear or logistic regression from scratch again!\n",
        "\n",
        "Let's dive in.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIDghAqLuq5S"
      },
      "source": [
        "## Statsmodels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSVxsiM6uq5S"
      },
      "source": [
        "`statsmodels` makes statistics in Python easy! The library contains tools for regressions ranging from linear regression, to logistic regression, count regressions (negative binomial and poisson), various options for robust covariance measures, and tools to implement time series models as well! There are also really useful tools for assisting in creating our regression model based on any structure that best suits us.\n",
        "\n",
        "We can import `statsmodels` in one of two ways:\n",
        "\n",
        "1) With support for R-style formulas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ktA32fgcuq5T"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLm0j15Wuq5U"
      },
      "source": [
        "    /opt/conda/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
        "      import pandas.util.testing as tm\n",
        "\n",
        "\n",
        "This is probably the best way to import our data if we are doing regression analysis for causal inference. In these cases, we are not typically trying to make predictions as new data arrives, and so we do not need to have tools ready to analyze new data using our existing regression models.\n",
        "\n",
        "2) Import `statsmodels` to use pre-built numpy arrays as inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mf9DGU5Ruq5U"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYqMM1Oauq5U"
      },
      "source": [
        "In this case, we have other tools that we can use, but we need to manually arrange our `x` and `y` matrices. It looks clunky at first, but can be useful when we are building predictive pipelines using regression models, or when we might want to use both `statsmodels` and `sklearn` with the same data source.\n",
        "\n",
        "Let's start with option 1..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvVU5ajcuq5U"
      },
      "source": [
        "### Preparing a Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx2T_-l5uq5V"
      },
      "source": [
        "When using formulas, we prepare our dataset by importing the data into a Pandas `DataFrame`. We should take care that each of our variables has a name with\n",
        "1) **No spaces**\n",
        "2) No symbols\n",
        "3) Made up of letters and numbers (also can't have a number as the first character)\n",
        "\n",
        "Our code so far might look something like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VzrCFD6duq5V"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8320/blob/master/AssignmentData/assignment8Data.csv?raw=true\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sekN3MHluq5V"
      },
      "source": [
        "Assuming that our data set has already been cleaned. If our data has not yet been cleaned, then we need to clean our data prior to working with either `statsmodels` or `sklearn`. This is because regression AND machine learning models require that all information be provided in numeric format. We need to transform text-based data into categorical data (using either ordered numeric columns or binary variable columns generated from our categories), and ensure that all data is represented in the way that we want to use it within our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdaRQzmQuq5V"
      },
      "source": [
        "### Regression Equations\n",
        "\n",
        "`statsmodels` incorporates `R`-style regression equations by using the `patsy` library behind the scenes. We will talk more about `patsy` soon. The pattern for regression equations is as follows:\n",
        "\n",
        "```\"dependent variable ~ independent variable + another independent variable + any other independent variables\"```\n",
        "\n",
        "The regression equation will be stored in a string (unlike in `R`), and we put our dependent variable (also called the endogenous variable, or outcome of interest) in the leftmost position within the string. We separate the dependent variable from all independent (exogenous or explanatory) variables using the `~` symbol. Then, each independent variable is separated from the others using `+` operators.\n",
        "\n",
        "The reason is is so important that our column names be properly cleaned before implementing regression analysis is that spaces and other problematic formats for column names will cause problems with our regression equations.\n",
        "\n",
        "### Implementing a Model\n",
        "\n",
        "The first model we might try is a simple linear regression. These are the most common regression models, and typically what someone is referring to when they discuss \"running a regression\". The code is wonderfully simple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMXLN4sBuq5W",
        "outputId": "b5a28567-6535-4b61-ab2a-a4d5438a05ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:               hhincome   R-squared:                      -0.000\n",
            "Model:                            OLS   Adj. R-squared:                 -0.000\n",
            "Method:                 Least Squares   F-statistic:                       nan\n",
            "Date:                Fri, 15 Nov 2024   Prob (F-statistic):                nan\n",
            "Time:                        17:16:12   Log-Likelihood:            -1.7131e+05\n",
            "No. Observations:               13712   AIC:                         3.426e+05\n",
            "Df Residuals:                   13711   BIC:                         3.426e+05\n",
            "Df Model:                           0                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      0.0188      0.000    138.414      0.000       0.019       0.019\n",
            "year          37.8564      0.274    138.414      0.000      37.320      38.392\n",
            "==============================================================================\n",
            "Omnibus:                     9819.620   Durbin-Watson:                   1.027\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           250725.793\n",
            "Skew:                           3.151   Prob(JB):                         0.00\n",
            "Kurtosis:                      22.978   Cond. No.                     9.31e+17\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The smallest eigenvalue is 6.41e-26. This might indicate that there are\n",
            "strong multicollinearity problems or that the design matrix is singular.\n"
          ]
        }
      ],
      "source": [
        "reg = smf.ols(\"hhincome ~ year\", data=data).fit()\n",
        "print(reg.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvMPyd1guq5W"
      },
      "source": [
        "                                OLS Regression Results                            \n",
        "    ==============================================================================\n",
        "    Dep. Variable:               hhincome   R-squared:                      -0.000\n",
        "    Model:                            OLS   Adj. R-squared:                 -0.000\n",
        "    Method:                 Least Squares   F-statistic:                      -inf\n",
        "    Date:                Wed, 16 Mar 2022   Prob (F-statistic):                nan\n",
        "    Time:                        15:24:28   Log-Likelihood:            -1.7131e+05\n",
        "    No. Observations:               13712   AIC:                         3.426e+05\n",
        "    Df Residuals:                   13711   BIC:                         3.426e+05\n",
        "    Df Model:                           0                                         \n",
        "    Covariance Type:            nonrobust                                         \n",
        "    ==============================================================================\n",
        "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
        "    ------------------------------------------------------------------------------\n",
        "    Intercept      0.0188      0.000    138.414      0.000       0.019       0.019\n",
        "    year          37.8564      0.274    138.414      0.000      37.320      38.392\n",
        "    ==============================================================================\n",
        "    Omnibus:                     9819.620   Durbin-Watson:                   1.027\n",
        "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):           250725.793\n",
        "    Skew:                           3.151   Prob(JB):                         0.00\n",
        "    Kurtosis:                      22.978   Cond. No.                     9.31e+17\n",
        "    ==============================================================================\n",
        "    \n",
        "    Warnings:\n",
        "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
        "    [2] The smallest eigenvalue is 6.41e-26. This might indicate that there are\n",
        "    strong multicollinearity problems or that the design matrix is singular.\n",
        "\n",
        "\n",
        "    /opt/conda/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:1657: RuntimeWarning: divide by zero encountered in double_scalars\n",
        "      return self.ess/self.df_model\n",
        "\n",
        "\n",
        "When we run these two lines of code, we are creating, fitting, and reporting on a regression model! It's fast, it's clean, and it's really easy to implement! `sm.ols` is the OLS class of regression models, and takes two required arguments: a regression equation (passed as a string), and a data source (expected to be a `pandas.DataFrame` object). We use the `.fit()` method to complete all of the math that actually solves our regression model. When we call `.summary()` on a fitted regression, we get a printout of the regression summary tables for the model, complete with diagnostic measures, estimates of our beta coefficients, and confidence intervals!\n",
        "\n",
        "If the model is satisfactory, then we are done! (It really is that simple!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KMpYM1Kuq5W"
      },
      "source": [
        "If I want to keep iterating on my model, I might want to try regressing year on the logged average household incomes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNbk677Ouq5W",
        "outputId": "2bbc396e-c6cc-4679-bb08-0cd919d19db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:       np.log(hhincome)   R-squared:                      -0.000\n",
            "Model:                            OLS   Adj. R-squared:                 -0.000\n",
            "Method:                 Least Squares   F-statistic:                       nan\n",
            "Date:                Fri, 15 Nov 2024   Prob (F-statistic):                nan\n",
            "Time:                        17:16:17   Log-Likelihood:                -17363.\n",
            "No. Observations:               13653   AIC:                         3.473e+04\n",
            "Df Residuals:                   13652   BIC:                         3.474e+04\n",
            "Df Model:                           0                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept   2.698e-06   1.82e-09   1481.190      0.000    2.69e-06     2.7e-06\n",
            "year           0.0054   3.67e-06   1481.190      0.000       0.005       0.005\n",
            "==============================================================================\n",
            "Omnibus:                     5172.537   Durbin-Watson:                   1.277\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            63678.616\n",
            "Skew:                          -1.469   Prob(JB):                         0.00\n",
            "Kurtosis:                      13.164   Cond. No.                     8.12e+17\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The smallest eigenvalue is 8.4e-26. This might indicate that there are\n",
            "strong multicollinearity problems or that the design matrix is singular.\n"
          ]
        }
      ],
      "source": [
        "reg = smf.ols(\"np.log(hhincome) ~ year\", data=data[data['hhincome']>0]).fit()\n",
        "print(reg.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVnEu2xjuq5W"
      },
      "source": [
        "                                OLS Regression Results                            \n",
        "    ==============================================================================\n",
        "    Dep. Variable:       np.log(hhincome)   R-squared:                      -0.000\n",
        "    Model:                            OLS   Adj. R-squared:                 -0.000\n",
        "    Method:                 Least Squares   F-statistic:                      -inf\n",
        "    Date:                Wed, 16 Mar 2022   Prob (F-statistic):                nan\n",
        "    Time:                        15:31:18   Log-Likelihood:                -17363.\n",
        "    No. Observations:               13653   AIC:                         3.473e+04\n",
        "    Df Residuals:                   13652   BIC:                         3.474e+04\n",
        "    Df Model:                           0                                         \n",
        "    Covariance Type:            nonrobust                                         \n",
        "    ==============================================================================\n",
        "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
        "    ------------------------------------------------------------------------------\n",
        "    Intercept   2.698e-06   1.82e-09   1481.190      0.000    2.69e-06     2.7e-06\n",
        "    year           0.0054   3.67e-06   1481.190      0.000       0.005       0.005\n",
        "    ==============================================================================\n",
        "    Omnibus:                     5172.537   Durbin-Watson:                   1.277\n",
        "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):            63678.616\n",
        "    Skew:                          -1.469   Prob(JB):                         0.00\n",
        "    Kurtosis:                      13.164   Cond. No.                     8.12e+17\n",
        "    ==============================================================================\n",
        "    \n",
        "    Warnings:\n",
        "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
        "    [2] The smallest eigenvalue is 8.4e-26. This might indicate that there are\n",
        "    strong multicollinearity problems or that the design matrix is singular.\n",
        "\n",
        "\n",
        "    /opt/conda/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:1657: RuntimeWarning: divide by zero encountered in double_scalars\n",
        "      return self.ess/self.df_model\n",
        "\n",
        "\n",
        "As you can see from the code above, everything is the same, except that we were able to transform household income using `np.log` on the go! We don't even need to create a new column! We can just do it inside of our regression model! We also subset our data so that the log operator doesn't break our model by introducing $-\\infty$ as a possible `hhincome` value.\n",
        "\n",
        "In other cases, it might be useful to create state-level fixed effects by including dummy variables for the states in our `statefip` column. Note that this won't work with our current data, since we only have one state in our data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1T77Lwjuq5W",
        "outputId": "93dee0d9-e38a-47c0-f8c1-268ab78ed5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:       np.log(hhincome)   R-squared:                      -0.000\n",
            "Model:                            OLS   Adj. R-squared:                 -0.000\n",
            "Method:                 Least Squares   F-statistic:                       nan\n",
            "Date:                Fri, 15 Nov 2024   Prob (F-statistic):                nan\n",
            "Time:                        17:16:23   Log-Likelihood:                -17363.\n",
            "No. Observations:               13653   AIC:                         3.473e+04\n",
            "Df Residuals:                   13652   BIC:                         3.474e+04\n",
            "Df Model:                           0                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept   2.698e-06   1.82e-09   1481.190      0.000    2.69e-06     2.7e-06\n",
            "year           0.0054   3.67e-06   1481.190      0.000       0.005       0.005\n",
            "==============================================================================\n",
            "Omnibus:                     5172.537   Durbin-Watson:                   1.277\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            63678.616\n",
            "Skew:                          -1.469   Prob(JB):                         0.00\n",
            "Kurtosis:                      13.164   Cond. No.                     8.12e+17\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The smallest eigenvalue is 8.4e-26. This might indicate that there are\n",
            "strong multicollinearity problems or that the design matrix is singular.\n"
          ]
        }
      ],
      "source": [
        "reg = smf.ols(\"np.log(hhincome) ~ year + C(statefip)\", data=data[data['hhincome']>0]).fit()\n",
        "print(reg.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC8dErG_uq5X"
      },
      "source": [
        "                                OLS Regression Results                            \n",
        "    ==============================================================================\n",
        "    Dep. Variable:       np.log(hhincome)   R-squared:                      -0.000\n",
        "    Model:                            OLS   Adj. R-squared:                 -0.000\n",
        "    Method:                 Least Squares   F-statistic:                      -inf\n",
        "    Date:                Wed, 16 Mar 2022   Prob (F-statistic):                nan\n",
        "    Time:                        15:32:19   Log-Likelihood:                -17363.\n",
        "    No. Observations:               13653   AIC:                         3.473e+04\n",
        "    Df Residuals:                   13652   BIC:                         3.474e+04\n",
        "    Df Model:                           0                                         \n",
        "    Covariance Type:            nonrobust                                         \n",
        "    ==============================================================================\n",
        "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
        "    ------------------------------------------------------------------------------\n",
        "    Intercept   2.698e-06   1.82e-09   1481.190      0.000    2.69e-06     2.7e-06\n",
        "    year           0.0054   3.67e-06   1481.190      0.000       0.005       0.005\n",
        "    ==============================================================================\n",
        "    Omnibus:                     5172.537   Durbin-Watson:                   1.277\n",
        "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):            63678.616\n",
        "    Skew:                          -1.469   Prob(JB):                         0.00\n",
        "    Kurtosis:                      13.164   Cond. No.                     8.12e+17\n",
        "    ==============================================================================\n",
        "    \n",
        "    Warnings:\n",
        "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
        "    [2] The smallest eigenvalue is 8.4e-26. This might indicate that there are\n",
        "    strong multicollinearity problems or that the design matrix is singular.\n",
        "\n",
        "\n",
        "    /opt/conda/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:1657: RuntimeWarning: divide by zero encountered in double_scalars\n",
        "      return self.ess/self.df_model\n",
        "\n",
        "\n",
        "The `C()` command indicates that we would like to consider the `statefip` variable as a **C**ategorical variable, not a numeric variable. We can transform ANY column using the categorical operator. It is most useful when a column is text-based, or when a column is numeric but should not be treated as a count, ordinal, or continuous variable. We CAN use it on our dependent variable, but this will (unless our dependent variable was binary text data) break our regression model, which expects only a single dependent variable, rather than an array of dependent variables.\n",
        "\n",
        "Sometimes we want to include transformed variables in our model without creating a new column. The `I()` operator allows us to do just that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8OOxmHkuq5X",
        "outputId": "2e590373-2257-4f90-f0e2-f42bfb2356a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Square a variable using the I() function for\n",
        "#   mathematical transformations\n",
        "reg = smf.ols(\"np.log(hhincome) ~ age + I(age**2)\", data=data).fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ7pbwfXuq5X"
      },
      "source": [
        "In this case, we transform `age` by squaring it (maybe in preparation to create an age-earnings profile?). One line, simple syntax, what could be better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "eiez5Egsuq5X",
        "outputId": "bf707d5b-d8f4-4ab6-89ae-ed7ffbd9fabe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PatsyError",
          "evalue": "Error evaluating factor: NameError: name 'education' is not defined\n    np.log(hhincome) ~ I(age-education-5)\n                       ^^^^^^^^^^^^^^^^^^",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/compat.py\u001b[0m in \u001b[0;36mcall_and_wrap_exc\u001b[0;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/eval.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, expr, source_name, inner_namespace)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         return eval(code, {}, VarLookupDict([inner_namespace]\n\u001b[0m\u001b[1;32m    170\u001b[0m                                             + self._namespaces))\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'education' is not defined",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mPatsyError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b54c938fc98a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Combine variables using the I() function for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#   mathematical transformations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"np.log(hhincome) ~ I(age-education-5)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36mfrom_formula\u001b[0;34m(cls, formula, data, subset, drop_cols, *args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         tmp = handle_formula_data(data, None, formula, depth=eval_env,\n\u001b[0m\u001b[1;32m    204\u001b[0m                                   missing=missing)\n\u001b[1;32m    205\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesign_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/formula/formulatools.py\u001b[0m in \u001b[0;36mhandle_formula_data\u001b[0;34m(Y, X, formula, depth, missing)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_using_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             result = dmatrices(formula, Y, depth, return_type='dataframe',\n\u001b[0m\u001b[1;32m     64\u001b[0m                                NA_action=na_action)\n\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/highlevel.py\u001b[0m in \u001b[0;36mdmatrices\u001b[0;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \"\"\"\n\u001b[1;32m    308\u001b[0m     \u001b[0meval_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvalEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     (lhs, rhs) = _do_highlevel_design(formula_like, data, eval_env,\n\u001b[0m\u001b[1;32m    310\u001b[0m                                       NA_action, return_type)\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/highlevel.py\u001b[0m in \u001b[0;36m_do_highlevel_design\u001b[0;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     design_infos = _try_incr_builders(formula_like, data_iter_maker, eval_env,\n\u001b[0m\u001b[1;32m    165\u001b[0m                                       NA_action)\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdesign_infos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/highlevel.py\u001b[0m in \u001b[0;36m_try_incr_builders\u001b[0;34m(formula_like, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformula_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelDesc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEvalEnvironment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         return design_matrix_builders([formula_like.lhs_termlist,\n\u001b[0m\u001b[1;32m     67\u001b[0m                                        formula_like.rhs_termlist],\n\u001b[1;32m     68\u001b[0m                                       \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/build.py\u001b[0m in \u001b[0;36mdesign_matrix_builders\u001b[0;34m(termlists, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;31m# on some data to find out what type of data they return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m     (num_column_counts,\n\u001b[0;32m--> 693\u001b[0;31m      \u001b[0mcat_levels_contrasts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_examine_factor_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_factors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m                                                    \u001b[0mfactor_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m                                                    \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/build.py\u001b[0m in \u001b[0;36m_examine_factor_types\u001b[0;34m(factors, factor_states, data_iter_maker, NA_action)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamine_needed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactor_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcat_sniffers\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mguess_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcat_sniffers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/eval.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, memorize_state, data)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemorize_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         return self._eval(memorize_state[\"eval_code\"],\n\u001b[0m\u001b[1;32m    569\u001b[0m                           \u001b[0mmemorize_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                           data)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/eval.py\u001b[0m in \u001b[0;36m_eval\u001b[0;34m(self, code, memorize_state, data)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemorize_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0minner_namespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVarLookupDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemorize_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"transforms\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         return call_and_wrap_exc(\"Error evaluating factor\",\n\u001b[0m\u001b[1;32m    552\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m                                  \u001b[0mmemorize_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_env\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/compat.py\u001b[0m in \u001b[0;36mcall_and_wrap_exc\u001b[0;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m                                  origin)\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Use 'exec' to hide this syntax from the Python 2 parser:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raise new_exc from e\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# In python 2, we just let the original exception escape -- better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mPatsyError\u001b[0m: Error evaluating factor: NameError: name 'education' is not defined\n    np.log(hhincome) ~ I(age-education-5)\n                       ^^^^^^^^^^^^^^^^^^"
          ]
        }
      ],
      "source": [
        "# Combine variables using the I() function for\n",
        "#   mathematical transformations\n",
        "reg = smf.ols(\"np.log(hhincome) ~ I(age-education-5)\", data=data).fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8OfAOQUuq5X"
      },
      "source": [
        "This example combines TWO columns to create a new measure (proxying experience by subtracting education from age, and subtracting an additional 5 years). All we have to do is describe the relationship that we want to model as an explanatory variable, and we are off to the races! Most operators are fair game, and we can include an arbitrary number of columns in our measure calculation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJBhXqUIuq5X"
      },
      "source": [
        "### More robust modeling\n",
        "\n",
        "If we want to utilize robust standard errors, we can easily update our regression results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ_vOHgyuq5X",
        "outputId": "98789018-8a9f-4994-b841-db43db1850a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:       np.log(hhincome)   R-squared:                         nan\n",
            "Model:                            OLS   Adj. R-squared:                    nan\n",
            "Method:                 Least Squares   F-statistic:                       nan\n",
            "Date:                Fri, 15 Nov 2024   Prob (F-statistic):                nan\n",
            "Time:                        17:17:00   Log-Likelihood:                    nan\n",
            "No. Observations:               13702   AIC:                               nan\n",
            "Df Residuals:                   13701   BIC:                               nan\n",
            "Df Model:                           0                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept        -inf        nan        nan        nan         nan         nan\n",
            "year             -inf        nan        nan        nan         nan         nan\n",
            "==============================================================================\n",
            "Omnibus:                          nan   Durbin-Watson:                     nan\n",
            "Prob(Omnibus):                    nan   Jarque-Bera (JB):                  nan\n",
            "Skew:                             nan   Prob(JB):                          nan\n",
            "Kurtosis:                         nan   Cond. No.                     1.66e+18\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The smallest eigenvalue is 2.02e-26. This might indicate that there are\n",
            "strong multicollinearity problems or that the design matrix is singular.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/regression/linear_model.py:1698: RuntimeWarning: invalid value encountered in subtract\n",
            "  return self.model.wendog - self.model.predict(\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/regression/linear_model.py:1733: RuntimeWarning: invalid value encountered in subtract\n",
            "  return np.sum(weights * (model.endog - mean)**2)\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/regression/linear_model.py:949: RuntimeWarning: invalid value encountered in subtract\n",
            "  resid = self.endog - np.dot(self.exog, params)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:1452: RuntimeWarning: invalid value encountered in subtract\n",
            "  a = op(a[slice1], a[slice2])\n"
          ]
        }
      ],
      "source": [
        "reg = smf.ols(\"np.log(hhincome) ~ year + C(statefip)\", data=data).fit()\n",
        "# Use White's (1980) Standard Error\n",
        "reg.get_robustcov_results(cov_type='HC0')\n",
        "print(reg.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVfYTJvhuq5X"
      },
      "source": [
        "Or, if we want to cluster our standard errors by state,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "ceju2QRIuq5Y",
        "outputId": "65c0cf5b-d626-4cfd-870d-4d382b758596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/regression/linear_model.py:1698: RuntimeWarning: invalid value encountered in subtract\n",
            "  return self.model.wendog - self.model.predict(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The weights and list don't have the same length.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2d784e9616da>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"np.log(hhincome) ~ year + C(statefip)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Use Cluster-robust Standard Errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_robustcov_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcov_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cluster'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'statefip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Need to specify groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36mget_robustcov_results\u001b[0;34m(self, cov_type, use_t, **kwargs)\u001b[0m\n\u001b[1;32m   2599\u001b[0m                     \u001b[0;31m# duplicate work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2600\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2601\u001b[0;31m                 res.cov_params_default = sw.cov_cluster(\n\u001b[0m\u001b[1;32m   2602\u001b[0m                     self, groups, use_correction=use_correction)\n\u001b[1;32m   2603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/stats/sandwich_covariance.py\u001b[0m in \u001b[0;36mcov_cluster\u001b[0;34m(results, group, use_correction)\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m     \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS_crosssection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0mnobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/stats/sandwich_covariance.py\u001b[0m in \u001b[0;36mS_crosssection\u001b[0;34m(x, group)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     '''\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0mx_group_sums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup_sums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m  \u001b[0;31m#TODO: why transposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mS_white_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_group_sums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/tools/grouputils.py\u001b[0m in \u001b[0;36mgroup_sums\u001b[0;34m(x, group, use_bincount)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         return np.array(\n\u001b[0;32m--> 106\u001b[0;31m             [\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/tools/grouputils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    105\u001b[0m         return np.array(\n\u001b[1;32m    106\u001b[0m             [\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             ]\n",
            "\u001b[0;31mValueError\u001b[0m: The weights and list don't have the same length."
          ]
        }
      ],
      "source": [
        "reg = smf.ols(\"np.log(hhincome) ~ year + C(statefip)\", data=data).fit()\n",
        "# Use Cluster-robust Standard Errors\n",
        "reg.get_robustcov_results(cov_type='cluster', groups=data['statefip']) # Need to specify groups\n",
        "print(reg.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38diblP9uq5Y"
      },
      "source": [
        "We don't have to stick to just `HC0` and cluster-robust standard errors. Below are some of the [covariance options](http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.get_robustcov_results.html) that we have:\n",
        "1) `HC0`: White's (1980) Heteroskedasticity robust standard errors\n",
        "2) `HC1`, `HC2`, `HC3`: MacKinnon and White's (1985) alternative robust standard errors, with `HC3` being designed for improved performance in small samples\n",
        "3) `cluster`: Cluster robust standard errors\n",
        "4) `hac-panel`: Panel robust standard errors\n",
        "\n",
        "We should choose the standard errors that best fit our specific data needs, and it is important to realize that this choice is highly context-dependent. The structure and nature of our data should be carefully considered, as should the specific regression model that we are trying to implement.\n",
        "\n",
        "### Time Series Models\n",
        "\n",
        "Not only can we model linear regression, we also have multiple time series options available. We won't go into much detail, since each of these models deserve to have significant time devoted to them, and we just don't have the time in this class.\n",
        "\n",
        "- [ARIMA](http://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html) models\n",
        "- [VAR](http://www.statsmodels.org/dev/generated/statsmodels.tsa.vector_ar.var_model.VAR.html) models\n",
        "- [Exponential Smoothing](https://www.statsmodels.org/stable/tsa.html#exponential-smoothing) models\n",
        "\n",
        "We can run an ARIMA, for example, using code like the following example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "fyd-yrcauq5Y",
        "outputId": "3e3d9d9e-c2a5-430c-8fd3-89406c7af2a7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "\nstatsmodels.tsa.arima_model.ARMA and statsmodels.tsa.arima_model.ARIMA have\nbeen removed in favor of statsmodels.tsa.arima.model.ARIMA (note the .\nbetween arima and model) and statsmodels.tsa.SARIMAX.\n\nstatsmodels.tsa.arima.model.ARIMA makes use of the statespace framework and\nis both well tested and maintained. It also offers alternative specialized\nparameter estimators.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-84f685894490>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'statefip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'hhincome'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mARIMA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hhincome'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/arima_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/arima_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARIMA_DEPRECATION_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: \nstatsmodels.tsa.arima_model.ARMA and statsmodels.tsa.arima_model.ARIMA have\nbeen removed in favor of statsmodels.tsa.arima.model.ARIMA (note the .\nbetween arima and model) and statsmodels.tsa.SARIMAX.\n\nstatsmodels.tsa.arima.model.ARIMA makes use of the statespace framework and\nis both well tested and maintained. It also offers alternative specialized\nparameter estimators.\n"
          ]
        }
      ],
      "source": [
        "# This won't work unless we have multiple years of data (which we currently don't)\n",
        "\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "\n",
        "y = data.loc[data['statefip']==31, ['hhincome','year']]\n",
        "y.index=pd.to_datetime(y.year)\n",
        "reg = ARIMA(y['hhincome'], order=(1,1,0)).fit()\n",
        "print(reg.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcCMBnSTuq5Y"
      },
      "source": [
        "### Modeling Discrete Outcomes\n",
        "\n",
        "If we have a [binary dependent variable](https://www.statsmodels.org/devel/discretemod.html), we are able to use either [Logit](https://www.statsmodels.org/devel/generated/statsmodels.discrete.discrete_model.Logit.html#statsmodels.discrete.discrete_model.Logit) or [Probit](https://www.statsmodels.org/devel/generated/statsmodels.discrete.discrete_model.Probit.html#statsmodels.discrete.discrete_model.Probit) models to estimate the effect of exogenous variables on our outcome of interest. To fit a Logit model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF8jCV_Iuq5Y",
        "outputId": "5b7af6c4-675a-4460-ed31-a2d087b4e10b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.615837\n",
            "         Iterations 6\n"
          ]
        }
      ],
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "myformula=\"married ~ hhincome + C(statefip) + C(year) + educ\"\n",
        "model= sm.Logit.from_formula(myformula, data=data).fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkazJ7Zyuq5Z"
      },
      "source": [
        "### Modeling Count Data\n",
        "\n",
        "When modeling count data, we have options such as [Poisson](http://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Poisson.html#statsmodels.discrete.discrete_model.Poisson) and [Negative Binomial](http://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.NegativeBinomial.html#statsmodels.discrete.discrete_model.NegativeBinomial) models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "ImepcjqZuq5Z",
        "outputId": "f0af677f-00fe-4ea0-e489-67f37a9417f4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "PatsyError",
          "evalue": "Error evaluating factor: NameError: name 'hhincome' is not defined\n    nchild ~ hhincome + C(statefip) + C(year) + educ + married\n             ^^^^^^^^",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/compat.py\u001b[0m in \u001b[0;36mcall_and_wrap_exc\u001b[0;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/eval.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, expr, source_name, inner_namespace)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         return eval(code, {}, VarLookupDict([inner_namespace]\n\u001b[0m\u001b[1;32m    170\u001b[0m                                             + self._namespaces))\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hhincome' is not defined",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mPatsyError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f5909abe6bc0>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmyformula\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nchild ~ hhincome + C(statefip) + C(year) + educ + married\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPoisson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_formula\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyformula\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36mfrom_formula\u001b[0;34m(cls, formula, data, subset, drop_cols, *args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         tmp = handle_formula_data(data, None, formula, depth=eval_env,\n\u001b[0m\u001b[1;32m    204\u001b[0m                                   missing=missing)\n\u001b[1;32m    205\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesign_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/formula/formulatools.py\u001b[0m in \u001b[0;36mhandle_formula_data\u001b[0;34m(Y, X, formula, depth, missing)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_using_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             result = dmatrices(formula, Y, depth, return_type='dataframe',\n\u001b[0m\u001b[1;32m     64\u001b[0m                                NA_action=na_action)\n\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/highlevel.py\u001b[0m in \u001b[0;36mdmatrices\u001b[0;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \"\"\"\n\u001b[1;32m    308\u001b[0m     \u001b[0meval_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvalEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     (lhs, rhs) = _do_highlevel_design(formula_like, data, eval_env,\n\u001b[0m\u001b[1;32m    310\u001b[0m                                       NA_action, return_type)\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/highlevel.py\u001b[0m in \u001b[0;36m_do_highlevel_design\u001b[0;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     design_infos = _try_incr_builders(formula_like, data_iter_maker, eval_env,\n\u001b[0m\u001b[1;32m    165\u001b[0m                                       NA_action)\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdesign_infos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/highlevel.py\u001b[0m in \u001b[0;36m_try_incr_builders\u001b[0;34m(formula_like, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformula_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelDesc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEvalEnvironment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         return design_matrix_builders([formula_like.lhs_termlist,\n\u001b[0m\u001b[1;32m     67\u001b[0m                                        formula_like.rhs_termlist],\n\u001b[1;32m     68\u001b[0m                                       \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/build.py\u001b[0m in \u001b[0;36mdesign_matrix_builders\u001b[0;34m(termlists, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;31m# on some data to find out what type of data they return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m     (num_column_counts,\n\u001b[0;32m--> 693\u001b[0;31m      \u001b[0mcat_levels_contrasts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_examine_factor_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_factors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m                                                    \u001b[0mfactor_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m                                                    \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/build.py\u001b[0m in \u001b[0;36m_examine_factor_types\u001b[0;34m(factors, factor_states, data_iter_maker, NA_action)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamine_needed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactor_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcat_sniffers\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mguess_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcat_sniffers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/eval.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, memorize_state, data)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemorize_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         return self._eval(memorize_state[\"eval_code\"],\n\u001b[0m\u001b[1;32m    569\u001b[0m                           \u001b[0mmemorize_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                           data)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/eval.py\u001b[0m in \u001b[0;36m_eval\u001b[0;34m(self, code, memorize_state, data)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemorize_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0minner_namespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVarLookupDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemorize_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"transforms\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         return call_and_wrap_exc(\"Error evaluating factor\",\n\u001b[0m\u001b[1;32m    552\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m                                  \u001b[0mmemorize_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_env\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/compat.py\u001b[0m in \u001b[0;36mcall_and_wrap_exc\u001b[0;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m                                  origin)\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Use 'exec' to hide this syntax from the Python 2 parser:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raise new_exc from e\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# In python 2, we just let the original exception escape -- better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patsy/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mPatsyError\u001b[0m: Error evaluating factor: NameError: name 'hhincome' is not defined\n    nchild ~ hhincome + C(statefip) + C(year) + educ + married\n             ^^^^^^^^"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8310/raw/master/DataSets/auto-mpg.csv\")\n",
        "\n",
        "myformula=\"nchild ~ hhincome + C(statefip) + C(year) + educ + married\"\n",
        "\n",
        "model= sm.Poisson.from_formula(myformula, data=data).fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DvKZjFxuq5Z"
      },
      "source": [
        "There are many other regression \"flavors\", and the best way to learn about what is available through `statsmodels` is to [read the docs](https://www.statsmodels.org/stable/user-guide.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyTcx-lauq5Z"
      },
      "source": [
        "## The `patsy` library\n",
        "\n",
        "We have been using regression equations in `statsmodels` a lot without really discussing what is happening behind the scenes. `statsmodels` relies on a library called `patsy` to parse regression equations and prepare our data for regression analysis. While `statsmodels` does a great job of incorporating the `patsy` library for us, this isn't always the case. In fact, it is a really valuable tool in many other contexts (think machine learning or deep learning).\n",
        "\n",
        "\n",
        "### Why use `patsy`?\n",
        "\n",
        "We don't necessarily have to use `patsy`. We could just select our variables manually. Creating a column of ones to serve as our intercept column is trivial (you of course remember that from the linear regression assignment). `patsy` is a tool for creating a standardized pipeline to deal with data that is stored in identical formats, and aids us in creating reusable or replicable code. Patsy allows us to separate our endogenous and exogenous variables AND to\n",
        "\t- \"Dummy out\" categorical variables\n",
        "\t- Easily transform variables (square, or log transforms, etc.)\n",
        "\t- Use identical transformations on future data\n",
        "    \n",
        "Even better, `patsy` is just as easy to use as regression equations. We just need to learn about the function wrappers that are necessary to create our processed data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Pr_b5YS3uq5Z"
      },
      "outputs": [],
      "source": [
        "import patsy as pt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8320/blob/master/AssignmentData/assignment8Data.csv?raw=true\")\n",
        "\n",
        "# To create y AND x matrices\n",
        "y, x = pt.dmatrices(\"hhincome ~ year + educ + married + age\", data = data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G39b6BL6uq5Z"
      },
      "source": [
        "In order to get started, we need to import `patsy`, and we typically give it the two-letter abbreviation `pt`. Once we have imported our data, we use the `pt.dmatrices` function. This function takes a regression equation (again, as a string), and a data source. The returned value is a **tuple** of `y` and `x`. We can break that tuple into two values by using the `y, x = ...` syntax, so that we have a `y` array and an `x` array.\n",
        "\n",
        "We don't have to create BOTH `y` and `x` data, though! We can use the `pt.dmatrix` function to just create an `x` matrix. Maybe we already have a dependent variable, and want to try out variations on our explanatory variables to see how each performs. In this case, our regression equation should have no column name to the left of the `~` symbol:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dYCXLhmXuq5a"
      },
      "outputs": [],
      "source": [
        "# To create ONLY an x matrix\n",
        "x = pt.dmatrix(\"~ year + educ + married + age\", data = data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZptG3-Fouq5b"
      },
      "source": [
        "One more note is that these regression equations automatically include an intercept term. If you do NOT want an intercept term (some regression models and most machine learning models don't use them), then you can add `-1` as an exogenous variable in your regression equation, in order to indicate that you want to eliminate the column of ones that make up the intercept column in our matrix of exogenous regressors.\n",
        "\n",
        "### Categorical Variables\n",
        "\n",
        "Again, we have the functions described in the regression section above available to us as we transform our data. We can create categorical variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XbcbxuMvuq5b"
      },
      "outputs": [],
      "source": [
        "# To create y AND x matrices\n",
        "eqn = \"hhincome ~ C(year) + educ + married + age\"\n",
        "y, x = pt.dmatrices(eqn, data = data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI_-n-k0uq5b"
      },
      "source": [
        "And (again) we can transform variables!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aR2pDwwOuq5b",
        "outputId": "34c921fd-12bf-4b44-bc46-692fbb4c26a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# To create y AND x matrices\n",
        "eqn = \"I(np.log(hhincome)) ~ C(year) + educ + married + age + I(age**2)\"\n",
        "y, x = pt.dmatrices(eqn, data = data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_UHCLkAuq5b"
      },
      "source": [
        "We can also use interaction operators. `*` will interact each value of two columns, and also include the original columns in the regression model. `:` will include only the interaction terms, while omitting the original columns. Check out the [explanation of formulas](https://patsy.readthedocs.io/en/latest/formulas.html) for more details.\n",
        "\n",
        "\n",
        "### SUPER IMPORTANT $\\rightarrow$ Same Transformation on New Data!\n",
        "\n",
        "Often, we will want to build a model with observed data that can make predictions about new observations as those observations are recorded. `patsy` provides a simple function to take the structure of one exogenous matrix and generate another identically structured matrix using new data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "tmq-wR20uq5b",
        "outputId": "a9cda787-f53c-4cc6-c805-a06fcdfbf1ba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataNew' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d1ff02ff14be>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# To create a new x matrix based on our previous version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxNew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_design_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesign_info\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataNew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataNew' is not defined"
          ]
        }
      ],
      "source": [
        "# To create a new x matrix based on our previous version\n",
        "xNew = pt.build_design_matrices([x.design_info], dataNew)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXq_DeW6uq5c"
      },
      "source": [
        "In other words, we can create a new matrix in the SAME SHAPE as our original `x` matrix by using the `build_design_matrices()` function in `patsy`.\n",
        "\n",
        "We pass a list containing the old design matrix information (because we can actually create many matrices simultaneously), as well as the new data from which to construct our new matrix.\n",
        "\n",
        "Why does recreating our `x` array matter? This process ensures that we always have the same number of categories in our categorical variables. A new, smaller subset of data that is freshly observed may not contain observations of every category, in which case an updated patsy matrix would not contain the correct number of columns! We are able to maintain consistency in our model, making our work replicable. Most importantly, this will streamline the use of `statsmodels` and `sklearn` in the same workflow!\n",
        "\n",
        "Speaking of `sklearn`..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhl1gm7Auq5c"
      },
      "source": [
        "## `sklearn`\n",
        "\n",
        "What `statsmodels` does for regression analysis, `sklearn` does for predictive analytics and machine learning. It is a truly fabulous library. `sklearn` is likely the most popular machine learning library, and has a standard API to make using the library VERY simple. Even better, it's documentation is some of the nicest documentation you will find anywhere, and contains incredible detail about how to implement models, as well as lessons about the \"how\" and \"why\" of using each model. You couldn't write a better textbook about machine learning than the documentation for `sklearn`.\n",
        "\n",
        "Below, we will briefly discuss some of the models that are most commonly utilized from `sklearn`. Details will be sparse. We are mostly focused on the code implementation of these models. More detail on how machine learning models work is provided in  Business Forecasting, and is outside the scope of this course.\n",
        "\n",
        "### Decision Tree Classification (and Regression)\n",
        "\n",
        "[Classification](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) and [Regression](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor) Trees (CARTs) are the standard jumping-off point for exploring machine learning. They are very easy to implement in `sklearn`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WodLQlpbuq5c",
        "outputId": "cb4e1301-b948-4834-c27e-4c26f8fac915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-sample accuracy: 0.9753162225224119\n"
          ]
        }
      ],
      "source": [
        "from sklearn import tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import patsy as pt\n",
        "\n",
        "data = pd.read_csv(\"https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/roomOccupancy.csv\")\n",
        "\n",
        "y, x = pt.dmatrices(\"Occupancy ~ CO2\", data=data)\n",
        "\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(x, y.squeeze())\n",
        "\n",
        "pred = clf.predict(x)\n",
        "\n",
        "print(\"In-sample accuracy: {}\".format(accuracy_score(y.squeeze(), pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSUK7KDruq5c"
      },
      "source": [
        "    In-sample accuracy: 0.9753162225224119\n",
        "\n",
        "\n",
        "### Support Vector Machines\n",
        "\n",
        "We also implement [Support Vector Machines](http://scikit-learn.org/stable/modules/svm.html#svm) for both [classification](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) and [regression](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_NeHnG9uq5c",
        "outputId": "5dba9158-e46b-4825-8564-972deab16bdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-sample accuracy: 0.9184575709198084\n"
          ]
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf = svm.SVC()\n",
        "clf = clf.fit(x, y.squeeze())\n",
        "\n",
        "pred = clf.predict(x)\n",
        "\n",
        "print(\"In-sample accuracy: {}\".format(accuracy_score(y.squeeze(), pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGWXQrlAuq5c"
      },
      "source": [
        "    /opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
        "      \"avoid this warning.\", FutureWarning)\n",
        "\n",
        "\n",
        "    In-sample accuracy: 0.9397028122313643\n",
        "\n",
        "\n",
        "Can you see the API pattern yet?\n",
        "\n",
        "### Random Forest Models\n",
        "\n",
        "Again, available in both [classification](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) and [regression](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor) flavors, these models are aggregations of many randomized Decision Trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pY87VLquq5c",
        "outputId": "74b08841-7d32-411f-96e5-ade64db721a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-sample accuracy: 0.9751934176593393\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=50)\n",
        "clf = clf.fit(x, y.squeeze())\n",
        "\n",
        "pred = clf.predict(x)\n",
        "\n",
        "print(\"In-sample accuracy: {}\".format(accuracy_score(y.squeeze(), pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2ZRRiuSuq5d"
      },
      "source": [
        "    In-sample accuracy: 0.9748250030701215\n",
        "\n",
        "\n",
        "There MUST be a pattern here...\n",
        "\n",
        "Of course there is! We import our classifier (or regressor), then create an instance of that object. We can name it `clf` or anything else that we prefer. From there, the process is the same:\n",
        "- Use the `.fit()` method, passing in the relevant data for our context\n",
        "- Create predictions using our fitted model with `.predict()` and new exogenous data (or the old data to test in-sample fit)\n",
        "- Measure the performance of our model with `accuracy_score`, or any other metric that can describe performance given a specific use case\n",
        "\n",
        "### More from `sklearn`\n",
        "\n",
        "Many other tools are also available to aid in the data cleaning process through `sklearn`. Some of these are:\n",
        "\n",
        "- [Principal Component Analysis (PCA)](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)\n",
        "- [Factor Analysis](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html#sklearn.decomposition.FactorAnalysis)\n",
        "- Many [Cross-Validation Algorithms](http://scikit-learn.org/stable/modules/cross_validation.html)\n",
        "- [Hyperparameter Tuning](http://scikit-learn.org/stable/modules/grid_search.html)\n",
        "   - Finding the correct parameters for a decision tree or random forest, for example\n",
        "- [Model Evaluation Tools](http://scikit-learn.org/stable/modules/model_evaluation.html)\n",
        "- [Plotting decision trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oAMLbHyuq5d"
      },
      "source": [
        "## Solve-it!\n",
        "\n",
        "Using the wage data provided here (https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/wagePanelData.csv), create a linear regression model to explain and/or predict wages. Your data set should be labeled `data` and your fitted model should be stored as `reg`. If you do not name the model correctly, you won't get any points!\n",
        "\n",
        "Please put all your code for this exercise in the cell labeled `#si-linear-regression` file found below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z1aMC6F9uq5d",
        "outputId": "ef967d39-2630-4448-e709-66987c55392f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:               log_wage   R-squared:                       0.384\n",
            "Model:                            OLS   Adj. R-squared:                  0.383\n",
            "Method:                 Least Squares   F-statistic:                     863.4\n",
            "Date:                Fri, 15 Nov 2024   Prob (F-statistic):               0.00\n",
            "Time:                        17:28:05   Log-Likelihood:                -1680.9\n",
            "No. Observations:                4165   AIC:                             3370.\n",
            "Df Residuals:                    4161   BIC:                             3395.\n",
            "Df Model:                           3                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "====================================================================================\n",
            "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------------\n",
            "const                5.1819      0.032    161.180      0.000       5.119       5.245\n",
            "year                 0.0869      0.003     30.408      0.000       0.081       0.093\n",
            "education            0.0738      0.002     35.717      0.000       0.070       0.078\n",
            "years_experience     0.0100      0.001     18.741      0.000       0.009       0.011\n",
            "==============================================================================\n",
            "Omnibus:                      136.778   Durbin-Watson:                   0.488\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              211.974\n",
            "Skew:                          -0.311   Prob(JB):                     9.34e-47\n",
            "Kurtosis:                       3.913   Cond. No.                         147.\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"ade5a59c-42f6-425f-881b-ad18c4bb125e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ade5a59c-42f6-425f-881b-ad18c4bb125e\")) {                    Plotly.newPlot(                        \"ade5a59c-42f6-425f-881b-ad18c4bb125e\",                        [{\"hovertemplate\":\"Age=%{x}\\u003cbr\\u003eWage=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"showlegend\":false,\"x\":[1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7],\"xaxis\":\"x\",\"y\":[5.56068,5.72031,5.99645,5.99645,6.06146,6.17379,6.24417,6.16331,6.21461,6.2634,6.54391,6.69703,6.79122,6.81564,5.65249,6.43615,6.54822,6.60259,6.6958,6.77878,6.86066,6.15698,6.23832,6.30079,6.35957,6.46925,6.56244,6.62141,6.43775,6.62007,6.63332,6.98286,7.04752,7.31322,7.29574,6.90575,6.90575,6.90776,7.00307,7.06902,7.52023,7.33889,6.1334,6.17379,6.21261,6.31355,6.37502,6.44572,6.52209,6.3315,6.40357,6.54391,6.56244,6.59167,6.81783,6.89163,6.55108,6.55108,6.80239,6.90776,7.09008,7.17012,7.20786,6.39693,6.43775,6.43775,6.43775,6.52209,6.61338,6.73934,6.65801,6.72623,6.80239,6.90776,7.03966,7.12769,7.24423,6.55108,6.62936,6.72263,6.7334,6.72263,6.95177,7.05272,6.90575,6.90073,7.17012,7.26543,7.25912,7.25488,7.40001,6.80239,6.80239,6.9256,7.00307,7.1309,7.12448,7.12287,5.94017,5.86079,5.99894,6.0845,6.39526,6.47697,6.47697,5.97889,5.78383,5.85793,5.95324,6.14633,6.17587,6.38856,6.62007,6.49224,6.77992,6.74524,7.09008,7.1309,7.1309,6.90575,6.90575,7.6009,7.64969,7.71869,7.78322,7.80384,6.38688,6.47389,6.54247,6.67582,6.7417,6.83195,6.93634,6.78333,6.85751,6.98472,7.0579,7.20564,7.10988,7.26683,6.05912,6.05209,5.99146,6.21461,6.32436,6.44254,6.44254,6.59987,6.59987,6.68461,6.72743,6.88244,6.95655,7.04752,6.90575,6.90575,7.49554,7.66153,7.86327,8.03916,7.89357,5.43808,5.52146,5.57973,5.66988,5.73657,5.84354,5.85793,6.52942,6.57925,6.71538,6.68461,6.79122,6.97541,6.90776,6.47697,6.55108,6.86693,6.85646,7.27932,7.54961,7.36708,6.01616,6.90575,7.04752,7.26193,7.09008,6.96319,7.31322,6.7334,6.69703,6.68461,6.74524,6.73102,6.80239,6.74524,6.90575,6.90575,7.06219,7.57558,7.67322,7.47307,7.56008,6.19644,6.29895,6.3315,6.36303,6.48158,6.57786,6.64379,6.90575,6.90575,7.31322,7.24423,7.48437,7.54961,7.61332,6.90575,6.90575,7.46737,7.40853,7.60589,8.04879,7.74066,6.90575,6.90575,7.17012,7.27932,7.24065,7.37776,7.46737,5.7589,5.86079,5.82305,5.94803,6.24804,6.20658,5.98394,6.51471,6.6107,6.39693,6.75227,6.88653,7.01212,7.02554,6.62007,6.77422,7.1309,7.08841,7.1309,6.90776,7.18007,6.90575,6.7417,7.00307,7.1309,7.14283,7.28276,7.40914,6.05209,6.05209,6.05209,6.36819,6.43615,6.54965,6.52209,6.47697,6.58617,6.47697,6.52942,6.80239,6.77992,6.80793,6.35784,6.49979,6.58203,6.69456,6.78784,6.90776,7.35819,6.90575,6.90575,7.37776,7.49109,7.57558,7.52833,7.745,6.2186,6.49375,6.58064,6.6896,6.79347,6.92461,7.02554,6.68461,6.84375,6.90776,7.00307,7.1309,7.40853,7.43838,6.90575,6.90575,7.20786,7.31322,7.43838,7.49332,7.6009,6.22456,6.33328,6.30992,6.47697,6.63332,6.68211,6.65286,6.12687,6.23245,6.32615,6.32436,6.4677,6.60665,6.55108,6.90575,6.90575,7.04752,7.09008,7.20786,7.11802,7.09008,6.90575,6.90575,7.1309,7.22621,7.1309,7.31322,7.43838,6.90575,6.90575,7.6009,7.67601,7.7021,7.69621,7.82084,6.27099,6.39859,6.48311,6.55678,6.65544,6.74876,6.84907,6.90575,6.90575,7.6009,7.64969,7.82405,7.86327,7.91936,6.90575,6.90575,7.24423,7.1309,7.46737,7.6009,7.64969,6.43615,6.90575,6.95655,6.95655,7.06048,7.20786,7.20786,6.51175,6.5903,6.68461,6.74524,6.8977,7.0775,7.10003,6.65415,6.85013,6.85646,6.92363,7.04141,7.1333,7.20786,6.10925,6.16331,6.33328,6.33328,6.43775,6.58617,6.53379,6.04025,6.28972,6.01616,6.57786,6.55108,6.53233,6.31355,6.55108,6.68461,6.69703,6.90776,7.00307,7.1624,7.1309,6.90575,6.90575,7.34601,7.43838,7.48156,7.6009,7.64969,5.97635,6.04025,6.04025,6.04025,6.1203,6.2186,6.1717,6.78559,6.88857,6.88755,6.96885,6.85646,6.73815,7.1309,6.58064,6.83518,6.91274,6.95845,6.9256,7.23778,7.2724,6.90575,6.90575,7.49554,7.54961,7.52564,7.72886,7.36645,6.35611,6.42162,6.47697,6.53669,6.55108,6.68461,6.80239,6.55536,6.62274,6.72263,6.77992,6.93828,7.03439,7.04578,6.3081,6.39693,6.51767,6.6107,6.68711,6.77079,6.85646,5.96615,6.02828,6.15273,6.21461,6.30992,6.44572,6.47697,6.45834,6.50728,6.56808,6.66823,6.75577,6.80239,6.95655,6.40523,6.55678,6.62007,6.62007,6.70319,6.83518,6.93049,6.47697,6.47697,6.62007,6.80239,6.90776,6.90776,7.17012,6.69703,6.80239,6.8977,6.95655,7.1309,7.24423,7.40184,6.47697,6.62007,6.62007,6.68461,6.85646,6.90776,7.00307,6.46147,6.65544,6.73459,6.81344,6.91473,6.8533,7.11314,5.4161,5.52146,5.59842,5.70378,5.76832,5.8861,5.85793,5.66988,5.99146,6.08677,6.17379,6.25575,6.43775,6.44572,5.16479,5.62762,5.84354,5.96101,6.02828,6.15273,6.24611,5.52146,5.70378,6.20456,6.05912,6.19848,6.16121,6.2804,5.70378,5.78383,5.78383,6.16331,6.21461,6.21461,6.30992,5.42935,5.47227,5.64545,5.68698,5.85793,5.95324,6.05209,6.74524,6.81674,6.65929,7.09008,7.07918,6.98472,7.26193,6.35437,6.33683,6.43775,6.39693,7.23346,7.35883,7.17319,6.62007,6.66568,6.81454,6.88857,6.9575,6.93342,7.06561,6.12249,6.08904,6.29157,6.39693,6.49072,6.59715,6.56386,6.58617,6.62007,6.62007,6.40192,6.77422,6.9256,6.70073,6.16331,6.22654,6.32077,6.4552,6.51471,6.62007,6.66823,6.21461,6.71538,6.43775,6.65673,6.65929,6.86485,6.65286,6.54247,6.60259,6.74641,6.86485,6.95845,7.01392,7.11558,6.3699,6.39693,6.55962,6.73102,6.86485,7.15462,7.2724,6.05209,6.02345,6.30992,6.38012,6.65929,6.68461,6.73934,6.21461,6.21461,6.30992,6.39693,6.21461,6.39693,6.65286,6.34388,6.41346,6.72743,6.76504,6.85646,7.00307,7.17012,6.47697,6.58617,6.7334,6.81344,6.9613,6.50279,6.70196,5.99146,6.17794,6.17794,6.39693,6.50129,6.60665,6.90776,6.79122,6.84588,6.87213,6.95655,7.09672,7.16085,7.16085,6.28413,6.32794,6.47697,6.50429,6.58755,6.76504,6.81014,6.90575,6.90575,7.34601,7.20786,7.52294,7.67601,7.78322,6.10925,6.22654,6.31355,6.41999,6.51471,6.62804,6.83195,6.72743,6.8977,6.98379,7.07918,7.17012,7.14756,7.36518,5.78074,5.826,5.84644,5.95584,6.06611,6.19032,6.23637,5.99146,6.10925,6.39693,6.39693,6.47697,6.47697,6.55108,6.69703,6.79682,6.89467,6.96885,6.9256,7.18007,7.17242,6.68461,6.80239,6.90776,7.04752,7.04752,7.31322,7.43838,6.30992,6.58617,6.62007,6.90776,6.96602,7.06902,6.95655,5.73657,5.81413,5.8999,5.94017,6.13123,6.13123,6.10925,6.67203,6.84268,6.84055,6.85646,6.99485,7.05618,7.29709,6.36303,6.47697,6.57228,6.65929,6.80239,6.58893,6.76849,6.90575,6.90575,7.1309,7.1309,7.31322,7.32975,7.49554,6.24611,6.21461,6.47697,6.49677,6.61473,6.74524,7.1309,6.74524,6.81344,6.87213,7.10661,7.24423,7.2998,7.37776,6.50728,6.55108,6.8773,6.55108,7.04752,6.83411,6.74524,6.10925,6.31355,6.45362,6.68461,6.79122,6.83518,6.96791,6.90575,6.90575,7.1107,7.25134,7.34601,7.34601,7.2998,6.90575,6.90575,7.17012,7.4553,7.56008,7.37212,7.75577,6.28972,6.3613,6.43615,6.43455,6.41182,6.3699,6.4677,6.69456,6.76849,6.90776,7.03966,7.02554,7.22766,7.21229,6.74052,6.82979,6.80239,6.94698,7.01031,7.13569,7.06817,6.57925,6.39693,6.62007,6.7093,6.76849,6.93537,7.04752,6.58617,6.68461,6.71538,6.77422,6.85646,6.91672,6.99942,6.90575,6.90575,8.08641,8.22951,7.90986,8.12089,8.537,6.24417,6.39693,6.90776,6.90776,6.77194,6.86693,6.90776,6.90575,6.90575,7.46737,7.52294,7.6009,7.50769,7.64969,6.62007,6.71417,6.81892,6.97635,6.92658,7.00307,7.00307,6.30992,6.55108,6.56526,6.74524,6.93925,6.95655,7.00307,6.8977,6.90575,7.17012,7.31322,7.37776,7.49554,7.49554,6.22059,6.30079,6.37673,6.50129,6.62007,6.78672,6.91672,6.41673,6.4892,6.6174,6.70564,6.85646,6.43775,5.7462,6.83518,6.90575,7.08171,7.09838,7.37776,7.43838,7.35244,5.52146,5.89715,6.35611,6.47697,6.54679,6.65673,6.76273,6.07074,6.12687,6.21461,6.27288,6.32794,6.44413,6.49072,6.48311,6.52503,6.608,6.71538,6.84055,6.96979,7.04752,6.48311,6.55393,6.59987,6.7093,6.85646,6.96791,7.06219,6.90575,6.90575,7.43838,7.49554,7.57558,7.64969,7.54697,6.32794,6.47697,6.58617,6.68461,6.68461,6.94119,7.04752,5.72031,5.95842,6.02102,6.21461,6.29157,6.37161,6.45362,6.65286,6.77422,7.09008,6.90776,7.10824,7.0282,7.31322,6.90575,6.57925,7.18311,7.31322,7.40853,7.38647,7.64779,6.63726,6.78559,6.91274,6.98286,6.95655,7.0193,7.25347,6.90575,6.90575,7.46737,7.50384,7.58579,7.66388,7.46737,6.43775,6.62007,6.62007,6.77422,7.1309,7.1507,7.24423,6.23048,6.33328,6.36475,6.45047,6.57228,6.99393,7.21891,5.8861,5.78383,5.99146,6.10925,6.32077,6.43775,6.50578,6.72143,6.68085,6.72623,6.8773,6.8773,6.9613,7.10906,6.37161,6.47697,6.58617,6.68461,6.81344,6.8773,6.96602,6.23048,6.30992,6.39693,6.62007,6.77422,6.74993,7.17012,6.77422,6.90575,7.31322,7.49554,6.91374,7.10988,7.39817,6.90575,6.90575,7.1309,7.17012,7.24423,7.31322,7.40853,6.47697,6.61338,6.72143,7.00307,7.00307,7.19818,7.21008,5.75257,5.75257,6.23441,6.47697,6.62007,6.95655,6.86693,6.53669,6.84162,6.68461,6.84055,6.90776,6.85646,6.90776,6.27099,6.53959,6.56667,6.59578,6.83841,6.94022,6.80351,6.30992,6.43294,6.48158,6.60665,6.63332,6.74524,6.82763,5.4161,5.45959,5.45959,5.57973,5.70378,5.70378,5.70378,6.22456,6.30992,6.30992,6.38519,6.43455,6.55108,6.68461,6.74524,6.72743,6.88244,6.92756,6.89972,7.02731,7.1309,6.52209,6.68461,6.80239,7.00307,7.00307,7.27932,7.37776,6.65286,6.77422,6.80239,6.85646,7.00307,7.09008,7.06902,6.54535,6.45362,6.57786,6.67456,6.77308,6.86901,6.90776,6.63857,6.68461,6.7334,6.88244,6.9256,7.04141,7.08841,6.55108,6.58617,6.65286,6.43775,6.62007,6.80239,6.68461,6.62007,6.80239,6.85646,6.90776,6.95655,7.09008,7.1507,6.10925,6.21461,6.21461,6.5582,6.66568,6.86171,6.97073,5.743,6.05209,6.07993,6.30992,6.39693,6.47697,6.51471,6.04025,6.18415,6.21461,6.30992,6.4552,6.52209,6.64249,6.07304,6.19441,6.31897,6.39526,6.47697,6.59715,6.69208,6.17794,6.31716,6.41182,6.39693,6.51471,6.61338,6.67834,6.71538,6.78559,6.88449,7.00307,7.09008,7.17012,7.29302,5.01064,5.01064,5.52146,5.66988,5.73657,5.81413,5.81413,6.23245,6.57925,6.65286,6.74524,7.49554,8.16052,8.3082,6.90575,6.90575,7.40853,7.46737,7.52294,7.46737,7.57558,5.67332,5.7462,5.82895,6.07993,5.99146,5.99146,6.13773,5.99146,5.92693,6.07993,6.07993,6.2634,6.43775,6.55108,6.90575,6.90575,7.22766,7.29302,7.33368,7.57353,7.27101,6.90575,6.90575,7.34601,7.31322,7.55486,7.71869,7.69621,6.08222,6.51323,6.62141,6.6995,6.79571,6.89669,6.99668,5.96615,6.11147,6.17379,6.22059,6.28972,6.36475,6.72743,6.43615,6.65929,6.74524,6.85646,6.90776,7.1333,7.21891,6.77422,6.82546,6.07993,6.90776,6.21461,6.70073,6.59304,6.10032,6.15698,6.24222,6.38351,6.42487,6.57228,6.75693,5.7462,5.92693,6.01616,5.94803,6.05209,6.32436,6.3699,6.28972,6.39693,6.54535,6.73102,7.49332,6.73102,6.98286,6.55108,6.86693,6.9256,7.06476,7.1309,7.43838,7.14441,5.85793,5.8861,5.99146,6.02828,6.10925,6.16331,6.21461,5.42053,5.51745,5.19296,5.34711,5.64191,5.65948,5.75257,6.55393,6.71538,6.62407,6.83087,6.95655,6.7901,7.09008,6.55108,6.62007,6.70564,6.78559,6.88857,7.0076,7.11233,6.80239,6.85646,6.98008,7.09589,7.48829,7.45124,7.47873,6.90575,6.90575,7.11639,7.07412,7.0579,7.02643,7.26263,6.62007,6.62007,6.83303,6.9256,7.02376,7.06305,7.18311,6.90575,6.90575,7.04752,7.09008,7.1309,6.99942,7.01481,6.09582,6.29157,6.40523,6.62007,6.71901,6.82546,6.95464,6.10032,6.29157,6.29157,6.37161,6.47389,6.54679,6.5903,6.286,6.4599,6.54965,6.64639,6.76849,6.84162,6.94794,6.90575,6.90575,7.36518,7.42476,7.46737,7.28139,7.49332,6.54965,6.83518,6.79794,6.80128,6.91075,6.69084,6.47697,6.44572,6.62007,6.69703,6.82979,6.90776,6.72263,6.72022,6.05209,6.21461,6.43775,6.58617,6.68461,6.74524,6.90776,5.53339,5.82008,5.8999,5.97635,6.05209,6.12905,6.21261,6.68461,6.68461,6.77422,6.84588,7.02554,7.08841,7.1309,6.39693,6.51915,6.60394,6.73815,6.84268,6.90776,7.09008,6.17379,6.33683,6.35437,6.39526,6.55108,6.68461,6.73102,6.16331,6.24222,6.36647,6.44254,6.51026,6.62407,6.71538,5.90808,5.92693,6.10702,6.26149,6.26149,6.31716,6.42487,6.07993,6.07993,6.13773,6.18826,6.18826,5.96615,6.01616,6.47697,6.53669,6.58617,6.67203,6.80239,6.93731,7.04752,6.54679,6.68461,6.90776,6.69456,6.90776,6.93828,7.04839,6.90575,6.90575,7.34278,7.47591,7.50108,7.21744,7.87284,6.90575,6.90575,7.31322,7.70661,6.98286,7.46737,7.91936,5.39363,5.52146,5.75257,5.75257,5.81413,5.95324,6.05209,6.55108,6.67203,6.66568,6.67203,6.71538,6.80239,6.8977,6.62007,6.62007,6.71538,6.80239,6.85646,6.90776,6.95655,6.77422,6.90575,6.90776,7.3696,7.45298,7.64969,7.54961,6.90575,6.90575,7.09008,7.20786,7.31322,7.20117,7.6009,6.90575,6.90575,7.00307,7.09008,7.1309,7.08171,7.24423,6.286,6.35437,6.43775,6.43775,6.47697,6.55108,6.55108,6.2634,6.17379,6.10925,6.10925,6.30992,6.21461,6.39693,6.85646,6.90575,7.04752,7.1309,7.24423,6.99485,7.54961,6.05209,6.05209,6.21461,6.33328,6.43775,6.19236,6.53233,6.31536,6.33328,6.40192,6.54103,6.54679,6.65028,6.68835,6.81344,6.80239,6.95655,7.08841,7.36455,7.49554,7.21082,6.47851,6.55393,6.62804,6.74524,6.82979,7.09008,7.05099,6.90575,6.90575,7.20786,7.29302,7.37776,7.15462,7.28207,5.70378,6.2634,5.92693,5.52146,5.99146,6.16331,6.35437,6.10925,6.24417,6.28227,6.38519,6.4892,6.55678,6.77422,6.90575,6.90575,7.03878,7.12287,7.12287,7.17012,7.30653,6.02345,6.25383,6.33328,6.47697,6.49072,6.84268,6.90174,5.57973,5.66988,5.826,5.87493,5.8999,6.10925,6.19236,6.90575,6.90575,7.1107,7.1309,6.67077,6.90776,6.90776,6.14633,6.55108,6.80239,6.90776,7.09008,7.24423,7.31322,6.68461,6.74524,6.74524,6.85646,6.90776,6.90776,6.90776,5.9135,6.24028,6.36303,6.55108,6.71538,6.80239,6.90776,6.90575,6.90575,7.64969,7.88231,7.82405,7.97247,8.04719,6.04025,6.90575,6.95655,7.01571,7.09008,7.1309,7.04141,6.63332,6.71538,6.82979,6.90776,7.00307,7.08255,7.1309,6.23245,6.30262,6.34564,6.47235,6.48768,6.71538,6.74524,5.92693,5.95324,6.10925,6.14204,6.17379,6.17379,6.17379,6.90575,6.90575,7.18917,7.19293,7.37776,7.37776,7.48717,6.16331,6.15273,6.3699,6.38012,6.31716,6.57925,6.59304,6.74524,6.85118,6.90776,6.96602,7.03439,6.92067,7.03527,6.90575,6.90575,7.82405,7.90101,7.67322,8.00637,8.05516,6.90575,6.90575,7.12367,7.23921,7.42357,7.46737,7.55747,6.53814,6.62936,6.72263,6.86485,6.87626,6.75809,6.76964,5.75257,5.75257,5.88053,5.94542,6.03548,6.02587,6.16542,6.90575,6.90575,7.1452,7.22548,7.33302,7.40853,7.34601,6.37161,6.46925,6.56948,6.51175,6.56526,6.64639,6.7334,6.39693,6.47697,6.55108,6.62007,6.66568,6.68461,6.7093,6.47697,6.35437,6.35437,6.50728,6.59715,6.66313,6.62007,5.43808,5.56068,5.57973,5.73657,5.76832,5.81413,6.04025,5.7462,5.73657,5.4161,5.46383,6.21461,5.85793,6.21461,6.62007,6.55108,6.74524,6.80239,6.95655,7.02554,7.00307,6.05678,6.10925,6.18415,6.28413,6.39693,6.51471,6.57508,5.8861,6.21461,6.44572,6.39693,6.65673,6.62007,6.62007,6.90575,6.90575,7.54961,7.6009,7.67322,7.6009,7.69621,6.23832,6.49224,6.52209,6.58617,6.80239,6.85646,6.95655,6.22654,6.30992,6.39693,6.52942,6.61204,6.70441,6.72623,6.05209,6.01616,6.27852,6.36819,6.45834,6.56244,6.63068,6.23048,6.35437,6.49979,6.68461,6.98008,7.00307,7.09008,6.64379,6.74406,6.79906,6.89366,7.02731,7.06476,7.20042,6.51175,6.49527,6.63726,6.63857,6.69456,6.69827,6.99393,5.85793,5.85793,6.21461,6.18415,6.23441,6.18415,6.16752,6.55108,6.63332,6.74524,6.95655,7.02554,7.1507,7.31322,6.31716,6.3613,6.42972,6.73102,6.81014,6.95177,6.9256,6.83841,6.85224,6.91175,6.91175,7.31122,7.18917,7.37776,6.10925,6.20456,6.37161,6.39693,6.55108,6.60259,6.62007,6.30992,6.29157,6.30992,6.65286,6.62007,6.57228,7.04752,6.67834,6.75227,6.84162,6.95655,7.00307,7.15462,7.21524,6.90575,6.90575,7.02997,7.09008,7.20786,7.26193,7.27932,6.55962,6.60123,6.73221,6.79122,6.90776,7.00307,7.09008,5.89715,6.07993,5.98141,6.33328,6.27476,6.20456,6.58617,6.44572,6.74524,6.81892,6.85646,7.00307,7.09008,7.20786,6.90575,6.90575,7.91936,8.13153,8.0229,7.92516,7.84502,6.4892,6.5582,6.57786,6.68461,6.39693,7.09008,6.86693,6.35437,6.44572,6.80239,6.80239,6.90776,6.96602,7.09008,6.31173,6.54535,6.56386,6.632,6.80239,6.84162,6.8977,6.64249,6.75344,6.81783,6.91473,6.95655,7.01571,7.09423,6.21461,6.17379,6.35437,6.55108,6.45362,6.21461,7.82405,6.55251,6.62007,6.68461,6.72383,6.82437,6.95655,7.00307,6.42162,6.4892,6.72022,6.76964,6.80239,6.8773,6.99577,6.90575,6.90575,7.17012,7.28276,7.36201,7.33498,7.47534,6.57368,6.6796,6.85857,6.84268,6.96791,7.1309,7.18992,6.05209,6.05209,6.10925,6.21461,6.2634,6.2634,6.30992,6.53088,6.55962,6.58064,6.72383,6.87626,6.97541,7.00307,6.74524,6.74524,6.81344,6.85118,7.02554,7.09008,7.27932,6.90575,6.21461,7.1309,6.90776,7.31322,7.46737,7.31322,6.44413,6.45362,6.52062,6.72743,6.59167,6.73815,6.77422,6.48311,6.53233,6.75227,6.80017,6.75926,6.96885,6.90975,5.99146,6.38012,6.56526,6.7334,6.74524,6.84588,6.74524,6.50429,6.47389,6.56386,6.7511,6.88755,6.99485,7.08423,6.86485,6.90575,7.02554,7.1507,7.24423,6.9594,7.44132,6.90575,6.90575,7.46737,7.69621,7.6009,7.91936,7.91936,6.28786,6.20456,6.24417,6.32794,6.53669,6.55108,6.62007,6.80239,6.82979,6.98472,7.00307,6.70808,6.9613,6.96035,6.61607,6.57647,6.68461,6.74288,7.03615,7.02465,7.22621,6.06611,6.18826,6.31173,6.55678,6.55393,6.75227,6.92166,6.19644,6.2653,6.36475,6.43935,6.54391,6.61473,6.71659,6.33328,6.61473,6.73459,6.77422,6.86485,6.97073,7.04752,6.44572,6.33328,6.39693,6.48158,6.59167,6.59304,6.77878,6.42972,6.43133,6.59441,6.71174,6.74524,6.8773,6.80239,6.49224,6.21461,6.72743,6.66568,6.30992,6.21461,6.21461,6.30992,6.30992,6.30992,6.30992,6.55108,6.62007,6.74524,6.14633,6.25383,6.30992,6.35089,6.46147,6.50279,6.59715,6.53669,6.55108,6.65929,6.82979,6.85646,6.95655,7.1309,6.22456,6.30992,6.47697,6.67834,6.77992,6.80461,6.85646,5.75257,5.85793,6.07535,8.51719,6.51471,6.58617,6.70319,6.16331,6.2634,6.35437,6.43775,6.47697,6.55108,6.40853,6.90575,6.90575,7.25134,7.25134,6.90776,7.23634,7.2724,6.49527,6.60394,6.70073,6.58617,6.41836,6.21461,6.62007,6.45047,6.67203,6.56386,6.60665,6.83518,6.86171,7.06987,6.48158,6.55393,6.61874,6.75693,6.87626,6.99026,7.07496,6.00389,6.10925,6.23441,6.03548,6.11589,6.19441,6.27099,6.3315,6.39359,6.48464,6.56948,6.70319,6.7901,6.87626,6.41182,6.56526,6.59987,6.69084,6.84055,6.93731,7.03615,6.57508,6.65157,6.75577,6.84055,6.90675,6.68336,7.06133,6.22258,6.34564,6.42972,6.49224,6.63595,6.65929,7.1309,6.30262,6.35611,6.4151,6.49224,6.62407,6.79571,6.97541,5.70711,6.39693,6.39693,6.63463,6.62007,6.68461,6.82979,6.90575,6.90575,7.67322,7.75148,7.81883,7.6009,7.90397,5.99146,5.90263,6.55108,6.49979,6.21461,6.68461,6.21461,6.35437,6.47697,6.74524,6.55108,6.80239,6.57925,6.90776,6.50129,6.57368,6.65544,6.75344,6.84801,6.94312,7.03527,6.09131,6.23637,6.27099,6.47697,6.59578,6.44572,6.77992,6.04025,6.20658,6.27288,6.39693,6.4708,6.54535,6.68211,6.68711,6.74524,6.85646,6.95655,7.04752,7.0934,7.1309,6.55108,6.55108,6.80239,6.85646,6.86693,7.00307,7.1309,6.18826,6.60665,6.68461,6.76849,6.86066,6.96602,7.00307,6.2634,6.29157,6.39693,6.39693,6.57786,6.62007,6.68461,6.55108,6.68461,6.55108,6.82871,6.80239,7.13648,7.18311,5.93225,6.19644,6.23637,6.30992,6.44254,6.10925,6.21461,6.62007,6.68461,6.7334,6.81892,6.87213,6.96602,7.04316,6.42162,6.47697,6.55108,6.68461,6.68461,6.80239,6.90776,6.75926,6.81344,6.86693,6.85118,6.9489,7.06476,7.11477,6.88959,6.90575,6.90776,7.00307,7.1309,7.10824,7.09008,5.61313,5.82008,5.76205,5.8999,5.95842,6.17379,6.10256,6.44572,6.62007,6.59987,6.63332,6.83518,6.99942,6.90776,6.33328,5.85793,6.39526,6.10925,6.10925,6.10925,6.39693,6.39526,6.43615,6.49224,6.73102,6.74524,7.04229,7.09008,6.30992,6.53669,6.46147,6.57925,6.8977,6.42487,6.91473,6.65286,6.77422,6.90776,6.96885,6.96224,7.10824,7.39326,5.98645,5.95584,6.18415,6.19236,6.62539,6.62007,6.80239,5.88332,5.8861,5.93489,6.07993,6.07993,6.08222,6.2634,6.57647,6.66441,6.73697,6.80793,6.80793,6.91771,7.04141,6.90575,6.90575,7.24566,7.37776,7.47307,7.57558,7.67322,6.4552,6.5582,6.66313,6.84588,6.9256,7.13648,7.27031,6.90575,6.90575,7.18539,7.24423,7.31122,7.37588,7.19519,6.90575,6.90575,7.6009,7.71869,7.74066,7.6009,7.6811,6.16331,6.30992,6.30445,6.62007,6.47697,6.55108,6.47697,6.90575,6.90575,6.57925,7.04752,7.09008,7.32251,7.31322,6.77422,6.88755,6.90776,6.96508,6.95655,7.19669,7.1309,6.90575,6.90575,7.84385,7.93737,8.10168,7.91936,8.02027,6.13988,6.56667,6.65157,6.83303,6.97167,7.07834,7.19669,6.55108,6.68711,6.75227,6.81344,6.96035,7.02019,7.08171,5.92693,5.99146,6.57368,6.66568,6.69703,6.82002,6.94505,6.43775,6.57368,6.65286,6.82655,6.90776,7.03615,7.04229,6.90575,6.90575,8.16052,8.00637,8.05516,7.94591,7.9831,6.50429,6.55108,6.64509,6.64639,6.90776,7.01392,7.09008,5.76832,5.8999,6.608,6.84588,6.08677,6.30992,6.30992,6.39024,6.49072,6.61874,6.80572,6.90375,6.98101,7.06219,6.34564,6.51471,6.57925,6.63332,6.80239,6.88755,7.0076,6.49072,6.51471,6.55108,6.70808,6.82002,6.4599,6.99118,5.98141,5.98141,6.10925,6.21461,6.25767,6.32436,6.43294,6.90575,6.90575,7.24423,6.95655,7.00307,7.17319,7.1309,5.4161,5.48064,6.21461,6.25383,6.34388,7.17012,7.21671,6.58617,6.66185,6.81344,6.86171,7.06902,7.0775,7.1107,6.12249,6.16542,6.21661,6.56526,6.62007,6.69332,6.80239,6.43775,6.39693,6.4708,6.74524,6.77422,6.7901,6.7901,6.90575,6.90575,5.01728,7.37776,7.46737,7.37776,7.33106,6.62007,6.55108,6.55108,5.87212,6.9256,6.76504,6.68211,6.51471,6.76273,6.93731,6.82979,6.88244,6.98008,7.09008,6.4677,6.62007,6.76849,6.99577,7.13648,7.37776,7.54961,6.29711,6.49224,6.74524,6.88244,6.95655,7.04752,7.1309,6.60259,6.62407,6.71538,6.84268,7.02554,7.2233,7.09008,6.56526,6.65929,6.78784,6.85435,6.96602,7.15383,7.10906,6.52209,6.58617,6.65286,6.65286,6.77765,6.80239,6.93049,6.05678,6.23441,6.39693,6.39859,6.39693,6.39693,6.47697,5.76832,5.85793,6.03787,6.10702,6.17379,6.23245,6.2672,5.68358,5.73334,5.8861,5.94542,6.01372,6.02345,6.10702,6.49224,6.55108,6.60665,6.68461,6.74524,6.80239,6.85646,6.82979,6.90575,6.90776,7.04752,7.20786,6.94986,7.40853,6.49224,6.65929,6.65673,6.80239,7.08171,7.15929,7.2724,6.49224,6.54391,6.50728,6.71538,6.73934,6.89264,6.95177,6.36303,6.40523,6.41346,6.42162,6.43775,6.62007,6.68461,6.37161,6.41673,6.39693,6.55108,6.90776,6.80239,6.90776,6.43775,6.43775,6.33683,6.37673,6.39359,6.71538,6.54391,6.80239,6.90575,6.96602,6.95655,7.09008,7.1309,6.9613,5.78383,6.12249,6.10925,6.10925,6.16331,6.19236,6.49677,6.60665,6.7334,6.79122,6.84588,6.95559,7.12287,7.16549,6.90575,6.90575,7.26193,7.27932,7.39326,7.35947,7.86327,6.56526,6.55108,6.63332,6.61607,6.80461,6.90776,7.18539,6.80239,6.90575,7.09008,7.1309,7.24423,7.31322,7.31322,6.90575,6.90575,7.11151,7.2717,7.33889,7.34601,7.59287,6.74759,6.84268,6.93731,7.03878,7.1309,7.22621,7.32317,6.29527,6.45047,6.39192,6.57786,6.56103,6.59715,6.65673,6.35957,6.39693,6.47697,6.08677,6.65929,6.79122,6.84055,6.44413,6.56808,6.63463,6.7334,6.83411,6.93537,7.01661,6.21461,6.21461,6.41346,6.50728,6.69703,6.69703,6.77422,6.05912,6.11147,6.27288,6.31173,6.41836,6.57925,6.66058,6.05209,6.10925,6.21461,6.30992,6.55108,6.55108,6.62007,6.77992,6.86693,6.8977,6.96885,7.05876,7.1309,6.98472,6.53669,6.65929,6.79122,6.88755,6.98472,7.09008,7.24423,6.38012,6.46147,6.54822,6.6733,6.77194,6.89467,6.95655,6.50877,6.60259,6.69332,6.75344,6.80128,6.95559,7.03351,6.58617,6.68586,6.78559,6.7334,7.6009,7.18311,7.49554,6.90575,6.90575,6.90776,7.1309,7.20786,7.44659,7.46737,6.90575,6.90575,7.16627,7.1309,7.37776,7.40853,7.47591,6.55108,6.68461,6.80239,6.82437,6.93245,7.00307,7.05186,6.90575,6.90575,7.1309,7.24423,7.46737,7.45008,7.64969,6.39693,6.48004,6.54965,6.65415,6.68461,6.82979,6.97541,6.80239,6.90575,7.09506,7.18007,7.27932,7.34601,7.46737,6.12249,6.14847,6.20658,6.30992,6.58341,6.58617,6.62672,6.38856,6.43775,6.47697,6.56386,6.44572,6.88244,6.96602,5.59842,5.70378,5.82895,6.05209,6.10925,6.16331,6.39693,6.02102,6.17379,6.30992,6.57925,6.65673,6.65028,6.9256,6.37161,6.72143,6.75344,7.24423,7.24423,7.1107,7.31322,6.55108,6.78672,6.88038,6.96885,7.03703,7.00125,7.27932,6.47697,6.56526,6.55108,6.55108,6.74524,6.86171,7.00307,6.05912,6.03548,6.07993,6.16121,6.31897,6.27099,6.44413,6.46459,6.53233,6.608,6.60394,6.80239,6.90675,7.0184,5.61677,6.00141,6.12687,6.1203,6.23048,6.30079,6.36647,6.49224,6.62936,6.66185,6.65929,6.75693,6.82437,6.90776,6.55108,6.63332,6.70319,6.79122,6.85646,7.06902,7.37776,6.39693,6.39693,6.47697,6.62007,6.74524,6.7901,6.88551,5.96615,6.10925,6.30992,6.39693,6.46925,6.51471,6.62007,6.2106,6.29157,6.41017,6.47697,6.57088,6.67708,6.77194,6.64639,6.69084,6.69084,6.86693,6.30992,6.81892,7.02909,6.82979,6.85646,6.90776,6.95655,7.00307,7.09008,7.18917,6.05209,6.62007,6.76849,6.77422,6.88244,6.97073,7.06902,6.4677,6.55108,6.55108,6.65929,6.68461,6.74524,6.86693,6.90575,6.90575,7.31255,7.31322,7.40853,7.39879,7.6256,6.10925,6.19236,6.25383,6.25383,6.42972,6.47697,6.55108,6.28227,6.54391,6.4892,6.58203,6.80239,6.88755,6.82979,5.4161,5.52146,5.61677,5.94017,6.06843,6.17794,6.2653,5.68017,5.81413,5.72359,6.21261,6.37843,6.4599,6.56386,6.90575,6.90575,7.03878,7.10661,7.21891,7.27932,7.31986,6.42162,6.42162,6.46614,6.84588,6.62007,6.62007,6.80239,6.54103,6.57925,6.64118,6.74052,6.85224,6.95655,7.06987,6.61338,6.6796,6.71538,6.86693,6.93245,7.00307,7.09008,6.47697,6.47697,6.55108,6.53233,6.62007,6.66823,6.17587,6.10032,6.22654,6.30445,6.34914,6.39359,6.39192,6.54822,6.24417,6.34564,6.42162,6.51471,6.58617,6.68461,6.7511,6.35437,6.45362,6.82111,6.94698,7.07242,7.07242,7.27932,6.10925,6.13773,6.05209,6.16331,6.30992,6.30992,6.47697,6.21461,6.21461,6.21461,6.30992,6.47697,6.62007,6.66823,5.95324,6.14633,6.24998,6.30992,6.47697,6.43775,6.64639,5.93225,6.06843,6.23245,6.29527,6.2804,6.35611,6.3081,6.68586,6.68461,6.74524,6.95655,6.90776,6.95655,7.09008,6.79122,6.90575,7.00307,7.09008,7.17012,7.20042,7.31322,5.8999,5.99146,6.10256,5.8944,6.44731,6.33683,6.47697,5.70378,5.78383,5.99146,5.99146,6.08677,6.16331,6.16331,6.52209,6.57925,6.7093,6.7334,6.88755,6.95655,7.04752,5.96615,6.08677,6.16331,6.85646,6.57647,7.10988,7.09008,6.48158,6.58617,6.66058,6.76964,6.88755,6.97354,7.08339,6.34212,6.5582,6.66568,6.7546,6.83303,6.93537,7.0076,5.94017,5.8861,6.06379,5.90808,6.24804,6.50279,6.4677,6.80239,6.83411,6.89366,6.89366,7.01392,7.17012,7.2998,6.52356,6.55393,6.55393,6.78333,7.04752,7.02554,7.20786,6.10925,6.20456,6.19441,6.28227,6.35437,6.49979,6.5582,6.51471,6.59578,6.66568,6.59987,6.88755,6.95273,6.95655,5.93754,6.0845,6.06146,6.21461,6.21461,6.30262,6.39693,6.51471,6.61338,6.71538,6.81892,6.95655,7.04752,6.85435,6.31716,6.6958,6.81564,6.87213,7.10661,7.09672,7.20042,6.49979,6.72743,6.71538,6.77878,6.91274,6.93245,7.40853,6.32794,6.44572,6.45362,6.50728,6.69703,6.81344,6.85646,6.16331,6.39693,5.95584,6.07535,6.2634,6.33683,6.42162,6.67834,6.68461,6.85646,6.85646,6.90776,7.04752,7.27932,5.92693,5.99146,5.85793,6.07993,6.21461,6.43775,6.55536,6.21461,6.38012,6.35437,6.47697,6.43775,6.52942,6.68461,6.24417,6.34564,6.42162,6.51471,6.58617,6.68461,6.7511,6.23441,6.31897,6.42162,6.49375,6.55108,7.00307,6.81014,6.90575,6.90575,7.31322,7.31322,7.31322,7.37776,7.37776,5.39363,5.43808,4.60517,5.0814,5.273,5.81413,5.74939,6.52942,6.62007,6.4599,7.00307,7.20786,7.46737,7.14913,6.65286,6.73102,6.82979,6.93245,7.02554,7.1309,7.20786,5.57973,5.73334,5.76832,5.86647,6.06611,6.23441,6.38688,6.42162,6.37502,6.58617,6.76273,6.74876,6.80239,6.93925,6.71538,6.74524,6.88244,6.98008,7.24423,7.43838,7.17702,6.81344,6.88244,7.00307,7.17012,7.17778,7.24423,7.39326,5.8693,5.85793,5.87774,5.85793,6.58617,6.35437,6.58617,6.2804,6.44572,6.48004,6.77422,6.83195,6.90776,7.00307,6.25383,6.46147,6.53669,6.62007,6.63595,6.7511,6.85646,6.90575,6.90575,7.23418,7.2998,7.31322,7.37588,7.61874,5.59842,6.21461,6.10925,6.2634,6.55108,6.80239,6.95655,5.70378,5.72031,5.99146,6.05209,6.21461,6.2634,6.30992,6.35437,6.35437,6.53669,6.60665,6.60665,6.75693,6.95177,6.07535,6.35437,6.58617,6.58893,6.59304,6.84268,6.97261,6.56103,6.90575,6.7334,6.8742,7.03439,7.03527,7.19669,6.35611,6.49224,6.65929,6.56948,6.80017,6.73102,6.9256,6.90575,6.90575,6.95655,7.02554,7.09008,7.09008,7.24423,6.72743,6.80239,6.88653,6.95655,7.09008,7.26193,7.42833,6.55251,6.62407,6.78672,6.83087,7.04491,7.02643,7.13569,6.5903,6.62007,6.65286,6.71538,6.77422,6.85646,6.93245,6.80239,6.90575,6.90776,7.01481,7.03878,6.9489,7.19669,6.27288,6.46303,6.53088,6.6695,6.66696,7.02554,7.02554,6.15698,6.10925,6.33683,6.35957,6.58755,6.71296,5.84932,6.55108,6.62007,6.70073,6.77194,6.85646,7.09008,6.97261,5.94803,6.05209,6.11589,6.18002,6.25958,6.31536,6.36819,6.21461,6.30992,6.68461,6.74288,6.81344,6.80239,6.95655,6.90575,6.90575,7.18463,7.28893,7.3908,7.48661,7.5606,6.90575,6.90575,7.09008,7.24423,7.35436,7.29097,7.46737,6.2634,6.30079,6.39693,6.39693,6.5582,6.77422,6.86901,6.35957,6.45677,6.61338,6.63988,6.77422,6.79122,6.84055,6.62007,6.62007,6.68461,6.88755,6.85646,5.98896,6.12687,6.35957,6.45205,6.52942,6.63857,6.79122,6.95655,7.03439,6.64639,6.71538,6.77422,6.95655,6.95655,7.20786,7.18917,6.80239,6.90575,7.1025,7.12769,7.18311,7.35372,7.40001,6.49072,6.64509,6.80239,6.73102,7.03703,6.49072,7.31122,6.90575,6.90575,7.43838,7.6009,6.77422,7.62071,7.79028,6.35437,6.43775,6.55108,6.68461,6.74524,6.68461,6.80239,6.21461,6.30992,6.37161,6.39693,6.55108,6.55108,6.71538,6.28227,6.51471,6.58617,6.71538,6.7511,6.4552,6.7093,6.39526,6.49072,6.59578,6.64118,6.80239,6.9187,6.96035,6.90575,6.90575,7.17012,7.27932,7.31322,7.37776,7.34923,6.19441,6.2634,6.29157,6.13123,6.74524,6.95655,6.84055,6.16331,6.39693,6.71538,6.58617,6.81344,6.90776,7.00307,6.03787,6.1717,6.13123,6.18826,5.76519,5.98896,5.99146,6.90575,6.90575,6.95655,7.09008,7.04752,7.1309,7.34666,5.50126,5.50126,5.65249,5.82305,5.8999,6.05209,6.14204,6.77992,6.90575,6.98472,7.08841,6.86171,6.97635,6.98101,5.59842,5.59842,5.7462,5.96615,6.1203,6.21461,6.34564,5.93225,5.97126,6.11368,6.25767,6.36303,6.40523,6.47697,5.78383,5.99146,5.89715,5.83188,6.05209,6.2634,6.43455,6.37502,6.56948,6.53669,6.64249,6.22654,6.37502,6.89366,6.62007,6.80239,6.85646,6.98008,7.04316,6.86693,7.18917,5.76832,5.82895,5.76832,6.59851,6.6796,7.09008,6.85751,6.18208,6.2634,6.39359,6.44095,6.51471,6.66441,6.75344,6.21461,6.39693,6.39693,6.56526,6.70073,6.52209,6.55678,6.24611,6.3315,6.40853,6.48158,6.55962,6.67834,6.82437,5.70378,5.92693,6.10925,6.04025,6.35957,6.50429,6.57786,6.23441,6.39693,6.50728,6.65286,6.74524,7.1309,7.24423,6.09807,6.2691,6.34036,6.48768,6.57786,6.62007,6.85646,6.90575,6.90575,7.06048,7.09589,6.93342,7.46737,7.37776,6.90575,6.90575,7.1293,7.17012,7.27031,7.45991,7.45703,6.57647,6.77422,6.88244,7.09008,7.20786,7.49554,7.6009,5.73657,5.96615,6.07535,6.15698,6.2634,6.34739,6.44572,6.29157,6.39526,6.39693,6.39693,6.49072,6.5903,6.6174,5.8861,5.98394,5.98394,6.19848,5.78996,6.01127,6.11147,5.57973,5.66988,5.70378,5.82895,5.99146,6.07535,6.14204,6.69456,6.72743,6.8533,6.89669,7.1309,6.96508,7.15462,6.03787,6.55108,6.65673,6.85646,7.09008,6.74406,6.99485,6.62007,6.77422,6.77422,6.90776,7.01481,7.06219,7.15851,6.34564,6.44413,6.62007,6.95655,7.31122,7.38833,7.6009,6.90575,6.90575,7.55799,7.6009,7.6009,7.6009,7.80588,6.55536,6.55108,6.63332,6.59715,6.82437,6.90776,7.09008,6.53959,6.53669,6.69084,6.84375,6.88244,7.04752,7.06048,6.65929,6.69084,6.77422,6.68461,6.90776,6.94986,7.17012,6.55393,6.68461,7.1107,7.17396,7.01481,6.92264,7.19519,6.8977,6.90575,7.1309,7.20786,7.31322,7.09008,7.32647,6.63988,6.72143,6.78897,6.9256,6.95655,7.00307,7.09257,6.56667,6.77422,6.7334,6.78333,6.77878,6.98472,7.02997,6.85646,6.74524,6.85646,6.90776,7.1309,6.68461,6.72503,5.70378,5.99146,5.99146,5.99146,5.99146,6.39693,6.02102,6.81124,6.74993,6.80239,6.88551,6.98379,7.08171,7.18159,6.04025,5.8693,6.21461,6.43775,6.1717,6.55108,6.55108,6.35437,6.52209,6.52649,6.67203,6.62007,6.80239,6.76849,5.9135,5.8861,5.99146,6.2634,6.35437,6.55108,6.52209,5.82305,5.89715,5.96358,6.02828,6.10702,6.32972,6.22654,6.90575,6.90575,7.31322,7.34601,7.40853,7.52294,7.48324,6.59715,6.89264,6.98749,6.98286,7.30317,7.39388,7.30317,6.35957,6.42811,6.45677,6.58341,6.54391,6.89568,6.94409,5.8999,6.19236,6.03309,6.34388,6.41017,6.39526,6.46303,6.31897,6.38182,6.44889,6.51323,6.56948,6.65801,6.76388,6.30992,6.30992,6.39693,6.39693,6.39693,6.47697,6.68461,6.10925,6.07993,8.26873,6.51471,6.46925,6.79794,6.92756,6.43775,6.59304,6.68461,6.85646,7.09008,7.1025,7.27932,6.65673,6.74052,6.90776,6.90776,6.90776,6.93245,6.90776,6.57925,6.77422,6.84268,6.95655,7.02554,7.08423,7.15305,6.84268,6.90575,7.1309,7.27031,7.05359,6.9613,8.04719,6.35437,6.51471,6.2634,6.68461,7.14362,7.19519,7.24423,5.82895,5.82895,5.82895,5.84354,5.8999,6.04025,6.21461,6.42487,6.48004,6.58064,6.68461,6.80793,6.92363,6.98379,6.46614,6.62007,6.62007,6.90776,7.04752,7.25912,7.15929,5.81711,5.8861,5.99894,6.06611,6.14419,6.29157,6.36303,6.34564,6.47389,6.82437,6.8977,6.97073,7.04752,6.91274,5.70378,5.99146,6.21461,6.44572,6.39693,6.62007,6.22654,6.10925,6.21461,6.25958,6.30628,6.40688,6.48311,6.62007,6.28227,6.38856,6.4552,6.66696,6.83518,6.90776,6.97541,5.79606,5.98141,6.04025,6.1717,6.28972,6.39526,6.57786,6.21461,6.2634,6.30992,6.47697,6.39693,6.68461,6.68461,5.92693,5.99146,6.10925,6.21461,6.2634,6.39693,6.55108,6.39359,6.46147,6.44889,6.63726,6.75926,6.83518,6.91968,5.79606,5.92693,5.96615,6.17379,6.47697,6.70073,6.71296,6.29157,6.38856,6.46147,6.57925,6.60665,6.71538,6.99393,6.77422,6.77422,6.89264,6.95655,6.95655,7.18917,7.28619,5.70378,6.1181,6.39693,6.72743,6.78784,6.91771,6.96224,6.29342,6.51323,6.64898,6.88346,7.04752,7.1293,7.16858,6.21461,6.30992,6.65286,6.53233,6.60259,6.70073,6.65286,6.55108,6.58617,6.68461,6.77992,6.90776,7.05704,7.02731,5.68698,5.70378,5.84354,5.99146,6.06379,6.10925,5.99146,6.61874,6.70319,6.78897,6.88449,7.07834,7.11883,7.17931,5.43808,5.92693,6.05209,6.10702,6.57786,6.35611,6.80239,6.66696,6.73697,6.80461,6.87523,6.95081,7.07834,7.17012,6.33683,6.53379,6.62007,6.59304,6.81124,6.81344,6.87005,6.46459,6.47697,6.60935,6.55108,6.77079,6.85646,6.85646,6.49979,6.51619,6.65157,6.76849,6.83841,6.86797,7.06133,5.29832,5.4161,5.4161,5.4161,5.70378,5.70378,5.67675,6.42487,6.48004,6.57368,6.69084,6.78672,6.91968,7.04752,5.68698,5.85793,5.95324,6.06379,6.21461,6.29157,6.37161],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"line\":{\"color\":\"red\"},\"mode\":\"lines\",\"name\":\"Fitted Line\",\"x\":[1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7],\"y\":[5.963065397415649,6.059998066083116,6.156930734750583,6.25386340341805,6.350796072085517,6.447728740752984,6.54466140942045,6.380986918721734,6.477919587389201,6.574852256056668,6.671784924724134,6.768717593391601,6.865650262059068,6.962582930726535,6.214492551029981,6.311425219697448,6.408357888364915,6.505290557032381,6.602223225699848,6.699155894367315,6.796088563034782,6.31720214102868,6.414134809696147,6.511067478363614,6.60800014703108,6.704932815698547,6.801865484366014,6.898798153033481,6.549728755849088,6.646661424516555,6.743594093184022,6.840526761851489,6.937459430518955,7.034392099186422,7.131324767853889,6.414735286147206,6.511667954814673,6.60860062348214,6.705533292149606,6.802465960817073,6.89939862948454,6.996331298152007,6.304601781832732,6.401534450500199,6.498467119167666,6.595399787835133,6.692332456502599,6.789265125170066,6.886197793837533,6.237105046981791,6.334037715649257,6.430970384316724,6.527903052984191,6.624835721651658,6.7217683903191245,6.818701058986591,6.479643798558059,6.576576467225526,6.673509135892993,6.77044180456046,6.867374473227927,6.964307141895393,7.06123981056286,6.479643798558059,6.576576467225526,6.673509135892993,6.77044180456046,6.867374473227927,6.964307141895393,7.06123981056286,6.394711012635483,6.49164368130295,6.588576349970417,6.685509018637884,6.782441687305351,6.879374355972817,6.976307024640284,6.3646746023678995,6.4616072710353665,6.5585399397028326,6.6554726083702995,6.7524052770377665,6.8493379457052335,6.9462706143727,6.414735286147206,6.511667954814673,6.60860062348214,6.705533292149606,6.802465960817073,6.89939862948454,6.996331298152007,6.673586354077311,6.770519022744778,6.867451691412245,6.964384360079711,7.061317028747178,7.158249697414645,7.255182366082112,6.392122790195395,6.489055458862862,6.585988127530329,6.682920796197796,6.779853464865262,6.876786133532729,6.973718802200196,6.314613918588593,6.41154658725606,6.508479255923527,6.605411924590993,6.70234459325846,6.799277261925927,6.896209930593394,6.462207747486424,6.559140416153891,6.656073084821358,6.753005753488824,6.849938422156291,6.946871090823758,7.043803759491225,6.699910807187007,6.796843475854474,6.893776144521941,6.990708813189407,7.087641481856874,7.184574150524341,7.281506819191808,6.554905200729263,6.65183786939673,6.748770538064197,6.845703206731664,6.94263587539913,7.039568544066597,7.136501212734064,6.699910807187007,6.796843475854474,6.893776144521941,6.990708813189407,7.087641481856874,7.184574150524341,7.281506819191808,5.8443842440676015,5.9413169127350685,6.038249581402535,6.135182250070002,6.232114918737469,6.329047587404936,6.425980256072402,6.502256294509869,6.599188963177336,6.696121631844803,6.793054300512269,6.889986969179736,6.986919637847203,7.08385230651467,6.55974089260495,6.656673561272417,6.753606229939884,6.85053889860735,6.947471567274817,7.044404235942284,7.141336904609751,6.204480414274119,6.301413082941586,6.398345751609053,6.49527842027652,6.592211088943986,6.689143757611453,6.78607642627892,6.603501396786282,6.700434065453749,6.797366734121216,6.894299402788683,6.9912320714561496,7.0881647401236165,7.185097408791083,6.515980388423618,6.612913057091085,6.7098457257585515,6.8067783944260185,6.9037110630934855,7.000643731760952,7.0975764004284185,6.529704482337365,6.626637151004832,6.723569819672299,6.820502488339766,6.917435157007233,7.0143678256747,7.111300494342166,6.414735286147206,6.511667954814673,6.60860062348214,6.705533292149606,6.802465960817073,6.89939862948454,6.996331298152007,6.612389798824343,6.70932246749181,6.806255136159276,6.903187804826743,7.00012047349421,7.097053142161677,7.193985810829144,6.324626055344455,6.421558724011922,6.518491392679388,6.615424061346855,6.712356730014322,6.809289398681789,6.906222067349256,6.74367131136834,6.840603980035807,6.937536648703273,7.03446931737074,7.131401986038207,7.228334654705674,7.325267323373141,6.749971490966313,6.84690415963378,6.943836828301247,7.040769496968714,7.13770216563618,7.234634834303647,7.331567502971114,6.589777302872533,6.68670997154,6.783642640207467,6.880575308874934,6.9775079775424,7.074440646209867,7.171373314877334,6.309778226712906,6.406710895380373,6.50364356404784,6.600576232715307,6.697508901382773,6.79444157005024,6.891374238717707,6.308313738990618,6.405246407658085,6.502179076325552,6.599111744993019,6.696044413660486,6.792977082327953,6.889909750995419,6.1570079529349,6.253940621602367,6.350873290269834,6.4478059589373,6.544738627604767,6.641671296272234,6.738603964939701,6.422159200462979,6.519091869130445,6.616024537797912,6.712957206465379,6.809889875132846,6.906822543800313,7.00375521246778,6.2545410980534255,6.3514737667208925,6.4484064353883594,6.5453391040558255,6.6422717727232925,6.7392044413907595,6.836137110058226,6.288289465478896,6.385222134146363,6.482154802813829,6.579087471481296,6.676020140148763,6.77295280881623,6.869885477483697,6.159596175374987,6.256528844042454,6.353461512709921,6.450394181377387,6.547326850044854,6.644259518712321,6.741192187379788,6.649850123407701,6.746782792075168,6.843715460742635,6.940648129410102,7.037580798077568,7.134513466745035,7.231446135412502,6.454783833170651,6.551716501838118,6.648649170505585,6.745581839173051,6.842514507840518,6.939447176507985,7.036379845175452,6.499668072069782,6.596600740737249,6.693533409404716,6.790466078072182,6.887398746739649,6.984331415407116,7.081264084074583,6.593489260030421,6.690421928697887,6.787354597365354,6.884287266032821,6.981219934700288,7.078152603367755,7.175085272035222,6.358374422769924,6.455307091437391,6.552239760104858,6.649172428772325,6.746105097439792,6.843037766107259,6.939970434774725,6.214492551029981,6.311425219697448,6.408357888364915,6.505290557032381,6.602223225699848,6.699155894367315,6.796088563034782,6.562329115045037,6.659261783712504,6.756194452379971,6.853127121047437,6.950059789714904,7.046992458382371,7.143925127049838,6.7099229439428685,6.8068556126103354,6.9037882812778015,7.0007209499452685,7.0976536186127355,7.194586287280202,7.291518955947669,6.759983627722175,6.856916296389642,6.953848965057109,7.050781633724575,7.147714302392042,7.244646971059509,7.341579639726976,6.464795969926512,6.561728638593979,6.658661307261445,6.755593975928912,6.852526644596379,6.949459313263846,7.046391981931313,6.833780542171091,6.930713210838558,7.027645879506025,7.124578548173491,7.221511216840958,7.318443885508425,7.415376554175892,6.699910807187007,6.796843475854474,6.893776144521941,6.990708813189407,7.087641481856874,7.184574150524341,7.281506819191808,6.358374422769924,6.455307091437391,6.552239760104858,6.649172428772325,6.746105097439792,6.843037766107259,6.939970434774725,6.210780593872093,6.3077132625395596,6.4046459312070265,6.5015785998744935,6.5985112685419605,6.695443937209427,6.7923766058768935,6.17962044888671,6.276553117554176,6.373485786221643,6.47041845488911,6.567351123556577,6.664283792224044,6.761216460891511,6.3846988758796215,6.4816315445470885,6.578564213214555,6.675496881882022,6.772429550549489,6.869362219216956,6.966294887884422,6.290877687918983,6.38781035658645,6.484743025253916,6.581675693921383,6.67860836258885,6.775541031256317,6.872473699923784,6.672462619359511,6.769395288026978,6.866327956694444,6.963260625361911,7.060193294029378,7.157125962696845,7.254058631364312,6.783719858391785,6.880652527059252,6.977585195726718,7.074517864394185,7.171450533061652,7.268383201729119,7.365315870396586,6.260841277651399,6.357773946318866,6.454706614986333,6.5516392836538,6.648571952321267,6.745504620988734,6.8424372896562,6.763695584880062,6.860628253547529,6.957560922214996,7.054493590882463,7.151426259549929,7.248358928217396,7.345291596884863,6.519692345581505,6.616625014248971,6.713557682916438,6.810490351583905,6.907423020251372,7.004355688918839,7.101288357586306,6.633537807053866,6.730470475721333,6.8274031443888,6.924335813056266,7.021268481723733,7.1182011503912,7.215133819058667,6.4948323801940955,6.5917650488615624,6.688697717529029,6.785630386196496,6.882563054863963,6.979495723531429,7.076428392198896,6.159596175374987,6.256528844042454,6.353461512709921,6.450394181377387,6.547326850044854,6.644259518712321,6.741192187379788,6.0594748078163745,6.1564074764838415,6.253340145151308,6.3502728138187745,6.4472054824862415,6.5441381511537084,6.641070819821175,6.434759559658929,6.531692228326395,6.628624896993862,6.725557565661329,6.822490234328796,6.919422902996263,7.016355571663729,6.4948323801940955,6.5917650488615624,6.688697717529029,6.785630386196496,6.882563054863963,6.979495723531429,7.076428392198896,6.498544337351983,6.595477006019449,6.692409674686916,6.789342343354383,6.88627501202185,6.983207680689317,7.080140349356784,6.529704482337365,6.626637151004832,6.723569819672299,6.820502488339766,6.917435157007233,7.0143678256747,7.111300494342166,6.519692345581505,6.616625014248971,6.713557682916438,6.810490351583905,6.907423020251372,7.004355688918839,7.101288357586306,6.633537807053866,6.730470475721333,6.8274031443888,6.924335813056266,7.021268481723733,7.1182011503912,7.215133819058667,6.579765166116672,6.676697834784139,6.7736305034516056,6.8705631721190725,6.9674958407865395,7.0644285094540065,7.1613611781214725,6.258253055211312,6.355185723878779,6.452118392546246,6.549051061213713,6.64598372988118,6.742916398548646,6.839849067216113,6.089511218083958,6.186443886751425,6.283376555418892,6.380309224086359,6.477241892753826,6.574174561421293,6.671107230088759,6.3183258757464795,6.4152585444139465,6.5121912130814135,6.60912388174888,6.706056550416347,6.8029892190838135,6.8999218877512805,6.106947269155594,6.203879937823061,6.300812606490528,6.397745275157995,6.494677943825461,6.591610612492928,6.688543281160395,6.52486879046168,6.621801459129147,6.718734127796613,6.81566679646408,6.912599465131547,7.009532133799014,7.106464802466481,5.959353440257762,6.056286108925229,6.153218777592696,6.250151446260162,6.347084114927629,6.444016783595096,6.540949452262563,6.573464986518698,6.670397655186165,6.767330323853632,6.864262992521098,6.961195661188565,7.058128329856032,7.155060998523499,6.509680208825643,6.60661287749311,6.703545546160576,6.800478214828043,6.89741088349551,6.994343552162977,7.091276220830444,6.374686739123761,6.471619407791227,6.568552076458694,6.665484745126161,6.762417413793628,6.859350082461095,6.956282751128562,6.274565371565148,6.3714980402326145,6.468430708900081,6.565363377567548,6.662296046235015,6.759228714902482,6.856161383569949,6.903865499462119,7.000798168129586,7.097730836797053,7.19466350546452,7.291596174131987,7.388528842799454,7.485461511466921,5.935617209588153,6.03254987825562,6.129482546923087,6.226415215590553,6.32334788425802,6.420280552925487,6.517213221592954,6.184456140762396,6.281388809429863,6.37832147809733,6.475254146764797,6.572186815432264,6.669119484099731,6.766052152767197,6.422159200462979,6.519091869130445,6.616024537797912,6.712957206465379,6.809889875132846,6.906822543800313,7.00375521246778,6.4258711576208665,6.522803826288333,6.6197364949558,6.716669163623267,6.8136018322907335,6.9105345009582,7.007467169625667,6.264553234809287,6.361485903476754,6.45841857214422,6.555351240811687,6.652283909479154,6.749216578146621,6.846149246814088,6.164431867250674,6.261364535918141,6.358297204585607,6.455229873253074,6.552162541920541,6.649095210588008,6.746027879255475,5.8855565258088465,5.982489194476313,6.07942186314378,6.176354531811247,6.2732872004787135,6.37021986914618,6.467152537813647,6.35207424317195,6.449006911839417,6.545939580506884,6.642872249174351,6.739804917841817,6.836737586509284,6.933670255176751,6.115835671193656,6.212768339861122,6.309701008528589,6.406633677196056,6.503566345863523,6.60049901453099,6.697431683198456,6.633537807053866,6.730470475721333,6.8274031443888,6.924335813056266,7.021268481723733,7.1182011503912,7.215133819058667,6.229681132666016,6.326613801333483,6.423546470000949,6.520479138668416,6.617411807335883,6.71434447600335,6.811277144670817,6.739959354210452,6.836892022877919,6.933824691545386,7.030757360212853,7.12769002888032,7.224622697547786,7.321555366215253,6.380986918721734,6.477919587389201,6.574852256056668,6.671784924724134,6.768717593391601,6.865650262059068,6.962582930726535,6.683598490833172,6.780531159500639,6.877463828168105,6.974396496835572,7.071329165503039,7.168261834170506,7.265194502837973,6.274565371565148,6.3714980402326145,6.468430708900081,6.565363377567548,6.662296046235015,6.759228714902482,6.856161383569949,6.160719910092786,6.257652578760253,6.35458524742772,6.451517916095187,6.548450584762654,6.64538325343012,6.742315922097587,6.59236552531262,6.689298193980087,6.786230862647554,6.883163531315021,6.980096199982487,7.077028868649954,7.173961537317421,6.553440713006975,6.650373381674442,6.747306050341909,6.844238719009376,6.941171387676843,7.038104056344309,7.135036725011776,6.589777302872533,6.68670997154,6.783642640207467,6.880575308874934,6.9775079775424,7.074440646209867,7.171373314877334,5.955641483099876,6.052574151767342,6.149506820434809,6.246439489102276,6.343372157769743,6.44030482643721,6.5372374951046766,6.412147063707118,6.509079732374585,6.606012401042051,6.702945069709518,6.799877738376985,6.896810407044452,6.993743075711919,6.214492551029981,6.311425219697448,6.408357888364915,6.505290557032381,6.602223225699848,6.699155894367315,6.796088563034782,6.464795969926512,6.561728638593979,6.658661307261445,6.755593975928912,6.852526644596379,6.949459313263846,7.046391981931313,6.304601781832732,6.401534450500199,6.498467119167666,6.595399787835133,6.692332456502599,6.789265125170066,6.886197793837533,6.404723149391344,6.501655818058811,6.598588486726278,6.695521155393745,6.792453824061211,6.889386492728678,6.986319161396145,6.52486879046168,6.621801459129147,6.718734127796613,6.81566679646408,6.912599465131547,7.009532133799014,7.106464802466481,6.288289465478896,6.385222134146363,6.482154802813829,6.579087471481296,6.676020140148763,6.77295280881623,6.869885477483697,6.418447243305092,6.515379911972559,6.612312580640026,6.709245249307493,6.80617791797496,6.903110586642426,7.000043255309893,6.793731995147646,6.890664663815112,6.987597332482579,7.084530001150046,7.181462669817513,7.27839533848498,7.375328007152447,6.813756268659368,6.910688937326835,7.007621605994302,7.104554274661769,7.201486943329236,7.298419611996702,7.395352280664169,6.29458964507687,6.391522313744337,6.488454982411804,6.585387651079271,6.682320319746738,6.779252988414204,6.876185657081671,6.442183473974701,6.539116142642168,6.636048811309635,6.732981479977102,6.829914148644569,6.926846817312035,7.023779485979502,6.663574217321449,6.760506885988916,6.857439554656383,6.95437222332385,7.051304891991316,7.148237560658783,7.24517022932625,6.183332406044597,6.280265074712064,6.377197743379531,6.474130412046998,6.571063080714464,6.667995749381931,6.764928418049398,6.683598490833172,6.780531159500639,6.877463828168105,6.974396496835572,7.071329165503039,7.168261834170506,7.265194502837973,6.29458964507687,6.391522313744337,6.488454982411804,6.585387651079271,6.682320319746738,6.779252988414204,6.876185657081671,6.579765166116672,6.676697834784139,6.7736305034516056,6.8705631721190725,6.9674958407865395,7.0644285094540065,7.1613611781214725,6.304601781832732,6.401534450500199,6.498467119167666,6.595399787835133,6.692332456502599,6.789265125170066,6.886197793837533,6.129559765107404,6.226492433774871,6.323425102442337,6.420357771109804,6.517290439777271,6.614223108444738,6.711155777112205,6.7299472174545905,6.826879886122057,6.923812554789524,7.020745223456991,7.117677892124458,7.214610560791925,7.311543229459391,6.29458964507687,6.391522313744337,6.488454982411804,6.585387651079271,6.682320319746738,6.779252988414204,6.876185657081671,6.184456140762396,6.281388809429863,6.37832147809733,6.475254146764797,6.572186815432264,6.669119484099731,6.766052152767197,6.5823533885567596,6.679286057224226,6.776218725891693,6.87315139455916,6.9700840632266265,7.0670167318940935,7.16394940056156,6.424747422903067,6.521680091570534,6.618612760238,6.715545428905467,6.812478097572934,6.909410766240401,7.006343434907868,6.1696083121308485,6.2665409807983155,6.3634736494657815,6.4604063181332485,6.5573389868007155,6.6542716554681824,6.751204324135649,6.1696083121308485,6.2665409807983155,6.3634736494657815,6.4604063181332485,6.5573389868007155,6.6542716554681824,6.751204324135649,6.159596175374987,6.256528844042454,6.353461512709921,6.450394181377387,6.547326850044854,6.644259518712321,6.741192187379788,6.7336591746124785,6.8305918432799455,6.9275245119474125,7.024457180614879,7.1213898492823455,7.2183225179498125,7.3152551866172795,6.344650328856177,6.441582997523644,6.538515666191111,6.6354483348585775,6.732381003526044,6.829313672193511,6.926246340860978,6.194468277518258,6.291400946185725,6.388333614853192,6.485266283520659,6.582198952188126,6.679131620855592,6.776064289523059,6.304601781832732,6.401534450500199,6.498467119167666,6.595399787835133,6.692332456502599,6.789265125170066,6.886197793837533,6.4284593800609535,6.5253920487284205,6.6223247173958875,6.719257386063354,6.8161900547308205,6.9131227233982875,7.0100553920657545,6.310901961430705,6.407834630098172,6.504767298765639,6.601699967433106,6.698632636100573,6.79556530476804,6.892497973435506,6.679886533675285,6.776819202342751,6.873751871010218,6.970684539677685,7.067617208345152,7.164549877012619,7.261482545680085,6.4896559353139205,6.5865886039813875,6.683521272648854,6.780453941316321,6.8773866099837875,6.9743192786512545,7.071251947318721,6.033150354706678,6.130083023374145,6.227015692041612,6.323948360709078,6.420881029376545,6.517813698044012,6.614746366711479,6.009414124037068,6.106346792704535,6.203279461372002,6.300212130039469,6.397144798706935,6.494077467374402,6.591010136041869,6.80003217474562,6.896964843413087,6.9938975120805535,7.0908301807480205,7.187762849415487,7.284695518082954,7.3816281867504205,6.4948323801940955,6.5917650488615624,6.688697717529029,6.785630386196496,6.882563054863963,6.979495723531429,7.076428392198896,6.372098516683673,6.469031185351139,6.565963854018606,6.662896522686073,6.75982919135354,6.856761860021007,6.953694528688473,6.639837986651839,6.736770655319306,6.833703323986773,6.93063599265424,7.027568661321707,7.124501329989173,7.22143399865664,6.736247397052566,6.833180065720033,6.9301127343874995,7.027045403054966,7.123978071722433,7.2209107403898996,7.3178434090573665,6.274565371565148,6.3714980402326145,6.468430708900081,6.565363377567548,6.662296046235015,6.759228714902482,6.856161383569949,6.342062106416089,6.438994775083556,6.535927443751023,6.63286011241849,6.729792781085957,6.826725449753423,6.92365811842089,6.509680208825643,6.60661287749311,6.703545546160576,6.800478214828043,6.89741088349551,6.994343552162977,7.091276220830444,6.314613918588593,6.41154658725606,6.508479255923527,6.605411924590993,6.70234459325846,6.799277261925927,6.896209930593394,6.649850123407701,6.746782792075168,6.843715460742635,6.940648129410102,7.037580798077568,7.134513466745035,7.231446135412502,5.899280619722594,5.996213288390061,6.093145957057528,6.190078625724995,6.287011294392462,6.383943963059928,6.480876631727395,6.284577508321009,6.381510176988476,6.478442845655943,6.57537551432341,6.672308182990877,6.769240851658344,6.86617352032581,6.643549943809727,6.7404826124771935,6.8374152811446605,6.934347949812127,7.031280618479594,7.128213287147061,7.225145955814528,6.479643798558059,6.576576467225526,6.673509135892993,6.77044180456046,6.867374473227927,6.964307141895393,7.06123981056286,6.55861715788715,6.6555498265546165,6.752482495222083,6.84941516388955,6.946347832557017,7.043280501224484,7.14021316989195,6.234516824541703,6.33144949320917,6.428382161876637,6.5253148305441036,6.6222474992115705,6.719180167879037,6.816112836546504,6.444771696414789,6.541704365082256,6.638637033749723,6.73556970241719,6.832502371084656,6.929435039752123,7.02636770841959,6.454783833170651,6.551716501838118,6.648649170505585,6.745581839173051,6.842514507840518,6.939447176507985,7.036379845175452,6.773707721635923,6.87064039030339,6.967573058970857,7.064505727638323,7.16143839630579,7.258371064973257,7.355303733640724,6.244528961297564,6.341461629965031,6.438394298632498,6.535326967299965,6.632259635967431,6.729192304634898,6.826124973302365,6.234516824541703,6.33144949320917,6.428382161876637,6.5253148305441036,6.6222474992115705,6.719180167879037,6.816112836546504,6.2545410980534255,6.3514737667208925,6.4484064353883594,6.5453391040558255,6.6422717727232925,6.7392044413907595,6.836137110058226,6.180744183604509,6.277676852271976,6.374609520939443,6.471542189606909,6.568474858274376,6.665407526941843,6.76234019560931,6.52486879046168,6.621801459129147,6.718734127796613,6.81566679646408,6.912599465131547,7.009532133799014,7.106464802466481,6.475931841400173,6.57286451006764,6.669797178735107,6.766729847402573,6.86366251607004,6.960595184737507,7.057527853404974,6.106947269155594,6.203879937823061,6.300812606490528,6.397745275157995,6.494677943825461,6.591610612492928,6.688543281160395,6.553440713006975,6.650373381674442,6.747306050341909,6.844238719009376,6.941171387676843,7.038104056344309,7.135036725011776,6.773707721635923,6.87064039030339,6.967573058970857,7.064505727638323,7.16143839630579,7.258371064973257,7.355303733640724,6.173320269288736,6.270252937956203,6.36718560662367,6.464118275291137,6.5610509439586036,6.65798361262607,6.754916281293537,6.274565371565148,6.3714980402326145,6.468430708900081,6.565363377567548,6.662296046235015,6.759228714902482,6.856161383569949,6.549728755849088,6.646661424516555,6.743594093184022,6.840526761851489,6.937459430518955,7.034392099186422,7.131324767853889,6.7299472174545905,6.826879886122057,6.923812554789524,7.020745223456991,7.117677892124458,7.214610560791925,7.311543229459391,6.204480414274119,6.301413082941586,6.398345751609053,6.49527842027652,6.592211088943986,6.689143757611453,6.78607642627892,6.184456140762396,6.281388809429863,6.37832147809733,6.475254146764797,6.572186815432264,6.669119484099731,6.766052152767197,6.342062106416089,6.438994775083556,6.535927443751023,6.63286011241849,6.729792781085957,6.826725449753423,6.92365811842089,6.566041072202924,6.662973740870391,6.759906409537858,6.856839078205325,6.953771746872791,7.050704415540258,7.147637084207725,5.963065397415649,6.059998066083116,6.156930734750583,6.25386340341805,6.350796072085517,6.447728740752984,6.54466140942045,6.649850123407701,6.746782792075168,6.843715460742635,6.940648129410102,7.037580798077568,7.134513466745035,7.231446135412502,6.264553234809287,6.361485903476754,6.45841857214422,6.555351240811687,6.652283909479154,6.749216578146621,6.846149246814088,6.29458964507687,6.391522313744337,6.488454982411804,6.585387651079271,6.682320319746738,6.779252988414204,6.876185657081671,6.3821106534395335,6.4790433221070005,6.5759759907744675,6.6729086594419345,6.769841328109401,6.8667739967768675,6.9637066654443345,5.959353440257762,6.056286108925229,6.153218777592696,6.250151446260162,6.347084114927629,6.444016783595096,6.540949452262563,6.404723149391344,6.501655818058811,6.598588486726278,6.695521155393745,6.792453824061211,6.889386492728678,6.986319161396145,6.3846988758796215,6.4816315445470885,6.578564213214555,6.675496881882022,6.772429550549489,6.869362219216956,6.966294887884422,6.504844516949957,6.601777185617424,6.698709854284891,6.795642522952358,6.892575191619824,6.989507860287291,7.086440528954758,6.374686739123761,6.471619407791227,6.568552076458694,6.665484745126161,6.762417413793628,6.859350082461095,6.956282751128562,6.2545410980534255,6.3514737667208925,6.4484064353883594,6.5453391040558255,6.6422717727232925,6.7392044413907595,6.836137110058226,6.753683448124201,6.8506161167916675,6.9475487854591345,7.044481454126601,7.141414122794068,7.238346791461535,7.3352794601290014,5.993101807683233,6.0900344763507,6.186967145018167,6.283899813685633,6.3808324823531,6.477765151020567,6.574697819688034,5.961941662697849,6.058874331365316,6.155807000032783,6.252739668700249,6.349672337367716,6.446605006035183,6.54353767470265,6.214492551029981,6.311425219697448,6.408357888364915,6.505290557032381,6.602223225699848,6.699155894367315,6.796088563034782,6.769995764478036,6.866928433145503,6.963861101812969,7.060793770480436,7.157726439147903,7.25465910781537,7.351591776482837,6.264553234809287,6.361485903476754,6.45841857214422,6.555351240811687,6.652283909479154,6.749216578146621,6.846149246814088,6.55974089260495,6.656673561272417,6.753606229939884,6.85053889860735,6.947471567274817,7.044404235942284,7.141336904609751,6.003113944439095,6.100046613106561,6.196979281774028,6.293911950441495,6.390844619108962,6.487777287776429,6.584709956443895,5.985677893367459,6.082610562034926,6.179543230702393,6.27647589936986,6.373408568037326,6.470341236704793,6.56727390537226,6.444771696414789,6.541704365082256,6.638637033749723,6.73556970241719,6.832502371084656,6.929435039752123,7.02636770841959,6.308313738990618,6.405246407658085,6.502179076325552,6.599111744993019,6.696044413660486,6.792977082327953,6.889909750995419,6.553440713006975,6.650373381674442,6.747306050341909,6.844238719009376,6.941171387676843,7.038104056344309,7.135036725011776,6.280865551163122,6.377798219830589,6.474730888498056,6.571663557165522,6.668596225832989,6.765528894500456,6.862461563167923,6.224504687785842,6.321437356453308,6.418370025120775,6.515302693788242,6.612235362455709,6.709168031123176,6.806100699790642,6.123259585509429,6.220192254176896,6.317124922844363,6.41405759151183,6.510990260179296,6.607922928846763,6.70485559751423,6.253417363335626,6.350350032003092,6.447282700670559,6.544215369338026,6.641148038005493,6.73808070667296,6.835013375340427,6.404723149391344,6.501655818058811,6.598588486726278,6.695521155393745,6.792453824061211,6.889386492728678,6.986319161396145,6.348362286014064,6.44529495468153,6.542227623348997,6.639160292016464,6.736092960683931,6.833025629351398,6.929958298018865,6.653562080565588,6.750494749233055,6.847427417900522,6.944360086567989,7.041292755235456,7.138225423902923,7.235158092570389,6.2308048673838154,6.327737536051282,6.424670204718749,6.521602873386216,6.618535542053682,6.715468210721149,6.812400879388616,6.753683448124201,6.8506161167916675,6.9475487854591345,7.044481454126601,7.141414122794068,7.238346791461535,7.3352794601290014,6.199644722398432,6.296577391065899,6.393510059733366,6.490442728400833,6.5873753970683,6.684308065735766,6.781240734403233,6.307190004272819,6.404122672940286,6.501055341607753,6.59798801027522,6.694920678942686,6.791853347610153,6.88878601627762,6.593489260030421,6.690421928697887,6.787354597365354,6.884287266032821,6.981219934700288,7.078152603367755,7.175085272035222,6.350950508454151,6.447883177121618,6.544815845789084,6.641748514456551,6.738681183124018,6.835613851791485,6.932546520458952,6.394711012635483,6.49164368130295,6.588576349970417,6.685509018637884,6.782441687305351,6.879374355972817,6.976307024640284,6.80003217474562,6.896964843413087,6.9938975120805535,7.0908301807480205,7.187762849415487,7.284695518082954,7.3816281867504205,6.693610627589034,6.7905432962565,6.887475964923967,6.984408633591434,7.081341302258901,7.178273970926368,7.275206639593835,6.268265191967173,6.36519786063464,6.462130529302107,6.559063197969574,6.65599586663704,6.752928535304507,6.849861203971974,6.264553234809287,6.361485903476754,6.45841857214422,6.555351240811687,6.652283909479154,6.749216578146621,6.846149246814088,6.344650328856177,6.441582997523644,6.538515666191111,6.6354483348585775,6.732381003526044,6.829313672193511,6.926246340860978,6.484820243438234,6.581752912105701,6.678685580773168,6.775618249440635,6.872550918108102,6.969483586775569,7.066416255443035,6.803744131903507,6.900676800570974,6.997609469238441,7.094542137905908,7.191474806573375,7.288407475240842,7.385340143908308,6.090634952801758,6.187567621469225,6.284500290136691,6.381432958804158,6.478365627471625,6.575298296139092,6.672230964806559,6.422159200462979,6.519091869130445,6.616024537797912,6.712957206465379,6.809889875132846,6.906822543800313,7.00375521246778,5.978253979051685,6.075186647719152,6.172119316386619,6.269051985054086,6.3659846537215525,6.462917322389019,6.559849991056486,6.2545410980534255,6.3514737667208925,6.4484064353883594,6.5453391040558255,6.6422717727232925,6.7392044413907595,6.836137110058226,6.454783833170651,6.551716501838118,6.648649170505585,6.745581839173051,6.842514507840518,6.939447176507985,7.036379845175452,6.739959354210452,6.836892022877919,6.933824691545386,7.030757360212853,7.12769002888032,7.224622697547786,7.321555366215253,6.4696316618021985,6.566564330469665,6.663496999137132,6.7604296678045985,6.8573623364720655,6.9542950051395325,7.0512276738069986,6.683598490833172,6.780531159500639,6.877463828168105,6.974396496835572,7.071329165503039,7.168261834170506,7.265194502837973,6.174444004006536,6.271376672674002,6.368309341341469,6.465242010008936,6.562174678676403,6.65910734734387,6.756040016011337,6.679886533675285,6.776819202342751,6.873751871010218,6.970684539677685,7.067617208345152,7.164549877012619,7.261482545680085,6.793731995147646,6.890664663815112,6.987597332482579,7.084530001150046,7.181462669817513,7.27839533848498,7.375328007152447,6.307190004272819,6.404122672940286,6.501055341607753,6.59798801027522,6.694920678942686,6.791853347610153,6.88878601627762,6.3646746023678995,6.4616072710353665,6.5585399397028326,6.6554726083702995,6.7524052770377665,6.8493379457052335,6.9462706143727,5.965653619855736,6.062586288523203,6.15951895719067,6.256451625858137,6.353384294525604,6.450316963193071,6.547249631860537,6.593489260030421,6.690421928697887,6.787354597365354,6.884287266032821,6.981219934700288,7.078152603367755,7.175085272035222,6.509680208825643,6.60661287749311,6.703545546160576,6.800478214828043,6.89741088349551,6.994343552162977,7.091276220830444,6.4748081066823735,6.57174077534984,6.6686734440173066,6.7656061126847735,6.8625387813522405,6.9594714500197075,7.056404118687174,6.71993508069873,6.816867749366196,6.913800418033663,7.01073308670113,7.107665755368597,7.204598424036064,7.301531092703531,6.4948323801940955,6.5917650488615624,6.688697717529029,6.785630386196496,6.882563054863963,6.979495723531429,7.076428392198896,6.1570079529349,6.253940621602367,6.350873290269834,6.4478059589373,6.544738627604767,6.641671296272234,6.738603964939701,6.07691085888801,6.173843527555476,6.270776196222943,6.36770886489041,6.464641533557877,6.561574202225344,6.658506870892811,6.244528961297564,6.341461629965031,6.438394298632498,6.535326967299965,6.632259635967431,6.729192304634898,6.826124973302365,6.227092910225929,6.324025578893396,6.420958247560862,6.517890916228329,6.614823584895796,6.711756253563263,6.80868892223073,6.145872081461239,6.242804750128706,6.339737418796173,6.436670087463639,6.533602756131106,6.630535424798573,6.72746809346604,6.377274961563848,6.474207630231314,6.571140298898781,6.668072967566248,6.765005636233715,6.861938304901182,6.958870973568649,5.875544389052985,5.972477057720452,6.069409726387919,6.166342395055386,6.263275063722853,6.360207732390319,6.457140401057786,6.180744183604509,6.277676852271976,6.374609520939443,6.471542189606909,6.568474858274376,6.665407526941843,6.76234019560931,6.749971490966313,6.84690415963378,6.943836828301247,7.040769496968714,7.13770216563618,7.234634834303647,7.331567502971114,6.187044363202483,6.28397703186995,6.380909700537417,6.477842369204884,6.574775037872351,6.671707706539818,6.768640375207284,6.509680208825643,6.60661287749311,6.703545546160576,6.800478214828043,6.89741088349551,6.994343552162977,7.091276220830444,6.773707721635923,6.87064039030339,6.967573058970857,7.064505727638323,7.16143839630579,7.258371064973257,7.355303733640724,6.224504687785842,6.321437356453308,6.418370025120775,6.515302693788242,6.612235362455709,6.709168031123176,6.806100699790642,6.484820243438234,6.581752912105701,6.678685580773168,6.775618249440635,6.872550918108102,6.969483586775569,7.066416255443035,6.184456140762396,6.281388809429863,6.37832147809733,6.475254146764797,6.572186815432264,6.669119484099731,6.766052152767197,6.499668072069782,6.596600740737249,6.693533409404716,6.790466078072182,6.887398746739649,6.984331415407116,7.081264084074583,6.4748081066823735,6.57174077534984,6.6686734440173066,6.7656061126847735,6.8625387813522405,6.9594714500197075,7.056404118687174,6.773707721635923,6.87064039030339,6.967573058970857,7.064505727638323,7.16143839630579,7.258371064973257,7.355303733640724,6.106947269155594,6.203879937823061,6.300812606490528,6.397745275157995,6.494677943825461,6.591610612492928,6.688543281160395,6.2545410980534255,6.3514737667208925,6.4484064353883594,6.5453391040558255,6.6422717727232925,6.7392044413907595,6.836137110058226,6.167020089690761,6.263952758358228,6.3608854270256945,6.4578180956931615,6.554750764360628,6.651683433028095,6.748616101695562,6.569753029360811,6.666685698028277,6.763618366695744,6.860551035363211,6.957483704030678,7.054416372698145,7.151349041365612,6.2545410980534255,6.3514737667208925,6.4484064353883594,6.5453391040558255,6.6422717727232925,6.7392044413907595,6.836137110058226,6.479643798558059,6.576576467225526,6.673509135892993,6.77044180456046,6.867374473227927,6.964307141895393,7.06123981056286,6.304601781832732,6.401534450500199,6.498467119167666,6.595399787835133,6.692332456502599,6.789265125170066,6.886197793837533,6.7336591746124785,6.8305918432799455,6.9275245119474125,7.024457180614879,7.1213898492823455,7.2183225179498125,7.3152551866172795,6.29458964507687,6.391522313744337,6.488454982411804,6.585387651079271,6.682320319746738,6.779252988414204,6.876185657081671,6.3821106534395335,6.4790433221070005,6.5759759907744675,6.6729086594419345,6.769841328109401,6.8667739967768675,6.9637066654443345,6.143283859021152,6.240216527688619,6.337149196356085,6.434081865023552,6.531014533691019,6.627947202358486,6.724879871025953,6.659862260163562,6.756794928831029,6.853727597498496,6.950660266165962,7.047592934833429,7.144525603500896,7.241458272168363,6.519692345581505,6.616625014248971,6.713557682916438,6.810490351583905,6.907423020251372,7.004355688918839,7.101288357586306,6.509680208825643,6.60661287749311,6.703545546160576,6.800478214828043,6.89741088349551,6.994343552162977,7.091276220830444,6.394711012635483,6.49164368130295,6.588576349970417,6.685509018637884,6.782441687305351,6.879374355972817,6.976307024640284,6.663574217321449,6.760506885988916,6.857439554656383,6.95437222332385,7.051304891991316,7.148237560658783,7.24517022932625,6.533416439495253,6.63034910816272,6.727281776830187,6.824214445497653,6.92114711416512,7.018079782832587,7.115012451500054,6.194468277518258,6.291400946185725,6.388333614853192,6.485266283520659,6.582198952188126,6.679131620855592,6.776064289523059,6.1896325856425705,6.286565254310037,6.383497922977504,6.480430591644971,6.577363260312438,6.674295928979905,6.771228597647371,6.445895431132589,6.542828099800055,6.639760768467522,6.736693437134989,6.833626105802456,6.930558774469923,7.027491443137389,6.304601781832732,6.401534450500199,6.498467119167666,6.595399787835133,6.692332456502599,6.789265125170066,6.886197793837533,6.518568610863705,6.615501279531172,6.712433948198639,6.809366616866106,6.9062992855335725,7.003231954201039,7.100164622868506,6.204480414274119,6.301413082941586,6.398345751609053,6.49527842027652,6.592211088943986,6.689143757611453,6.78607642627892,6.703622764344894,6.800555433012361,6.897488101679828,6.994420770347295,7.091353439014762,7.188286107682229,7.285218776349695,6.373563004405961,6.470495673073428,6.567428341740895,6.664361010408362,6.761293679075829,6.858226347743296,6.955159016410762,6.444771696414789,6.541704365082256,6.638637033749723,6.73556970241719,6.832502371084656,6.929435039752123,7.02636770841959,6.204480414274119,6.301413082941586,6.398345751609053,6.49527842027652,6.592211088943986,6.689143757611453,6.78607642627892,6.116959405911455,6.213892074578922,6.310824743246389,6.407757411913855,6.504690080581322,6.601622749248789,6.698555417916256,6.1570079529349,6.253940621602367,6.350873290269834,6.4478059589373,6.544738627604767,6.641671296272234,6.738603964939701,6.284577508321009,6.381510176988476,6.478442845655943,6.57537551432341,6.672308182990877,6.769240851658344,6.86617352032581,6.7299472174545905,6.826879886122057,6.923812554789524,7.020745223456991,7.117677892124458,7.214610560791925,7.311543229459391,6.268265191967173,6.36519786063464,6.462130529302107,6.559063197969574,6.65599586663704,6.752928535304507,6.849861203971974,6.514856653705818,6.611789322373285,6.708721991040752,6.805654659708218,6.902587328375685,6.999519997043152,7.096452665710619,6.424747422903067,6.521680091570534,6.618612760238,6.715545428905467,6.812478097572934,6.909410766240401,7.006343434907868,6.2545410980534255,6.3514737667208925,6.4484064353883594,6.5453391040558255,6.6422717727232925,6.7392044413907595,6.836137110058226,6.314613918588593,6.41154658725606,6.508479255923527,6.605411924590993,6.70234459325846,6.799277261925927,6.896209930593394,6.374686739123761,6.471619407791227,6.568552076458694,6.665484745126161,6.762417413793628,6.859350082461095,6.956282751128562,6.159596175374987,6.256528844042454,6.353461512709921,6.450394181377387,6.547326850044854,6.644259518712321,6.741192187379788,6.274565371565148,6.3714980402326145,6.468430708900081,6.565363377567548,6.662296046235015,6.759228714902482,6.856161383569949,6.264553234809287,6.361485903476754,6.45841857214422,6.555351240811687,6.652283909479154,6.749216578146621,6.846149246814088,6.80003217474562,6.896964843413087,6.9938975120805535,7.0908301807480205,7.187762849415487,7.284695518082954,7.3816281867504205,6.484820243438234,6.581752912105701,6.678685580773168,6.775618249440635,6.872550918108102,6.969483586775569,7.066416255443035,6.392122790195395,6.489055458862862,6.585988127530329,6.682920796197796,6.779853464865262,6.876786133532729,6.973718802200196,6.633537807053866,6.730470475721333,6.8274031443888,6.924335813056266,7.021268481723733,7.1182011503912,7.215133819058667,6.248240918455451,6.345173587122917,6.442106255790384,6.539038924457851,6.635971593125318,6.732904261792785,6.829836930460251,6.04316249146254,6.140095160130007,6.237027828797473,6.33396049746494,6.430893166132407,6.527825834799874,6.624758503467341,6.629825849895978,6.726758518563445,6.823691187230912,6.920623855898379,7.017556524565846,7.114489193233313,7.211421861900779,6.334638192100315,6.431570860767782,6.528503529435249,6.625436198102716,6.722368866770183,6.81930153543765,6.916234204105116,5.889268482966733,5.9862011516342,6.083133820301667,6.180066488969134,6.276999157636601,6.373931826304068,6.470864494971534,6.350950508454151,6.447883177121618,6.544815845789084,6.641748514456551,6.738681183124018,6.835613851791485,6.932546520458952,6.113247448753568,6.210180117421035,6.307112786088502,6.404045454755969,6.500978123423436,6.597910792090903,6.694843460758369,6.284577508321009,6.381510176988476,6.478442845655943,6.57537551432341,6.672308182990877,6.769240851658344,6.86617352032581,6.204480414274119,6.301413082941586,6.398345751609053,6.49527842027652,6.592211088943986,6.689143757611453,6.78607642627892,6.442183473974701,6.539116142642168,6.636048811309635,6.732981479977102,6.829914148644569,6.926846817312035,7.023779485979502,6.529704482337365,6.626637151004832,6.723569819672299,6.820502488339766,6.917435157007233,7.0143678256747,7.111300494342166,6.2545410980534255,6.3514737667208925,6.4484064353883594,6.5453391040558255,6.6422717727232925,6.7392044413907595,6.836137110058226,6.6235256702980045,6.7204583389654715,6.8173910076329385,6.9143236763004055,7.0112563449678715,7.1081890136353385,7.2051216823028055,6.82376840541523,6.920701074082697,7.017633742750164,7.114566411417631,7.211499080085097,7.308431748752564,7.405364417420031,6.274565371565148,6.3714980402326145,6.468430708900081,6.565363377567548,6.662296046235015,6.759228714902482,6.856161383569949,6.475931841400173,6.57286451006764,6.669797178735107,6.766729847402573,6.86366251607004,6.960595184737507,7.057527853404974,6.394711012635483,6.49164368130295,6.588576349970417,6.685509018637884,6.782441687305351,6.879374355972817,6.976307024640284,5.911880978918543,6.00881364758601,6.105746316253477,6.202678984920944,6.29961165358841,6.396544322255877,6.493476990923344,6.214492551029981,6.311425219697448,6.408357888364915,6.505290557032381,6.602223225699848,6.699155894367315,6.796088563034782,6.284577508321009,6.381510176988476,6.478442845655943,6.57537551432341,6.672308182990877,6.769240851658344,6.86617352032581,6.569753029360811,6.666685698028277,6.763618366695744,6.860551035363211,6.957483704030678,7.054416372698145,7.151349041365612,6.332049969660227,6.428982638327694,6.525915306995161,6.622847975662628,6.719780644330095,6.816713312997562,6.913645981665028,6.726235260296704,6.823167928964171,6.920100597631638,7.017033266299105,7.113965934966571,7.210898603634038,7.307831272301505,6.374686739123761,6.471619407791227,6.568552076458694,6.665484745126161,6.762417413793628,6.859350082461095,6.956282751128562,6.2545410980534255,6.3514737667208925,6.4484064353883594,6.5453391040558255,6.6422717727232925,6.7392044413907595,6.836137110058226,6.31720214102868,6.414134809696147,6.511067478363614,6.60800014703108,6.704932815698547,6.801865484366014,6.898798153033481,6.7562716705642885,6.8532043392317545,6.9501370078992215,7.0470696765666885,7.1440023452341554,7.240935013901622,7.3378676825690885,6.873829089194536,6.970761757862003,7.06769442652947,7.164627095196937,7.261559763864403,7.35849243253187,7.455425101199337,6.461084012768625,6.558016681436091,6.654949350103558,6.751882018771025,6.848814687438492,6.945747356105959,7.042680024773425,6.514856653705818,6.611789322373285,6.708721991040752,6.805654659708218,6.902587328375685,6.999519997043152,7.096452665710619,6.214492551029981,6.311425219697448,6.408357888364915,6.505290557032381,6.602223225699848,6.699155894367315,6.796088563034782,5.988266115807546,6.085198784475013,6.18213145314248,6.279064121809947,6.375996790477413,6.47292945914488,6.569862127812347,6.689898670431146,6.786831339098613,6.8837640077660796,6.9806966764335465,7.077629345101013,7.17456201376848,7.2714946824359465,6.5823533885567596,6.679286057224226,6.776218725891693,6.87315139455916,6.9700840632266265,7.0670167318940935,7.16394940056156,6.194468277518258,6.291400946185725,6.388333614853192,6.485266283520659,6.582198952188126,6.679131620855592,6.776064289523059,6.244528961297564,6.341461629965031,6.438394298632498,6.535326967299965,6.632259635967431,6.729192304634898,6.826124973302365,6.244528961297564,6.341461629965031,6.438394298632498,6.535326967299965,6.632259635967431,6.729192304634898,6.826124973302365,6.434759559658929,6.531692228326395,6.628624896993862,6.725557565661329,6.822490234328796,6.919422902996263,7.016355571663729,6.593489260030421,6.690421928697887,6.787354597365354,6.884287266032821,6.981219934700288,7.078152603367755,7.175085272035222,6.243405226579765,6.340337895247232,6.437270563914698,6.534203232582165,6.631135901249632,6.728068569917099,6.825001238584566,6.424747422903067,6.521680091570534,6.618612760238,6.715545428905467,6.812478097572934,6.909410766240401,7.006343434907868,6.713634901100756,6.810567569768223,6.90750023843569,7.004432907103157,7.1013655757706236,7.19829824443809,7.295230913105557,6.250829140895538,6.347761809563005,6.444694478230471,6.541627146897938,6.638559815565405,6.735492484232872,6.832425152900339,6.74367131136834,6.840603980035807,6.937536648703273,7.03446931737074,7.131401986038207,7.228334654705674,7.325267323373141,6.703622764344894,6.800555433012361,6.897488101679828,6.994420770347295,7.091353439014762,7.188286107682229,7.285218776349695,6.71993508069873,6.816867749366196,6.913800418033663,7.01073308670113,7.107665755368597,7.204598424036064,7.301531092703531,6.184456140762396,6.281388809429863,6.37832147809733,6.475254146764797,6.572186815432264,6.669119484099731,6.766052152767197,6.451071876012763,6.54800454468023,6.6449372133476965,6.7418698820151635,6.83880255068263,6.935735219350097,7.032667888017564,6.089511218083958,6.186443886751425,6.283376555418892,6.380309224086359,6.477241892753826,6.574174561421293,6.671107230088759,6.35207424317195,6.449006911839417,6.545939580506884,6.642872249174351,6.739804917841817,6.836737586509284,6.933670255176751,6.82376840541523,6.920701074082697,7.017633742750164,7.114566411417631,7.211499080085097,7.308431748752564,7.405364417420031,6.3646746023678995,6.4616072710353665,6.5585399397028326,6.6554726083702995,6.7524052770377665,6.8493379457052335,6.9462706143727,6.224504687785842,6.321437356453308,6.418370025120775,6.515302693788242,6.612235362455709,6.709168031123176,6.806100699790642,6.194468277518258,6.291400946185725,6.388333614853192,6.485266283520659,6.582198952188126,6.679131620855592,6.776064289523059,6.314613918588593,6.41154658725606,6.508479255923527,6.605411924590993,6.70234459325846,6.799277261925927,6.896209930593394,6.609801576384256,6.706734245051723,6.803666913719189,6.900599582386656,6.997532251054123,7.09446491972159,7.191397588389057,6.3672628248079866,6.4641954934754535,6.5611281621429205,6.658060830810387,6.7549934994778535,6.8519261681453205,6.9488588368127875,6.434759559658929,6.531692228326395,6.628624896993862,6.725557565661329,6.822490234328796,6.919422902996263,7.016355571663729,6.573464986518698,6.670397655186165,6.767330323853632,6.864262992521098,6.961195661188565,7.058128329856032,7.155060998523499,6.207068636714206,6.304001305381673,6.40093397404914,6.497866642716607,6.594799311384073,6.69173198005154,6.788664648719007,6.304601781832732,6.401534450500199,6.498467119167666,6.595399787835133,6.692332456502599,6.789265125170066,6.886197793837533,6.4284593800609535,6.5253920487284205,6.6223247173958875,6.719257386063354,6.8161900547308205,6.9131227233982875,7.0100553920657545,6.8437926789269525,6.9407253475944195,7.037658016261886,7.1345906849293526,7.2315233535968195,7.3284560222642865,7.4253886909317535,6.274565371565148,6.3714980402326145,6.468430708900081,6.565363377567548,6.662296046235015,6.759228714902482,6.856161383569949,6.183332406044597,6.280265074712064,6.377197743379531,6.474130412046998,6.571063080714464,6.667995749381931,6.764928418049398,6.539716619093227,6.636649287760694,6.733581956428161,6.830514625095628,6.927447293763095,7.024379962430561,7.121312631098028,6.322037832904367,6.418970501571833,6.5159031702393,6.612835838906767,6.709768507574234,6.806701176241701,6.903633844909168,6.244528961297564,6.341461629965031,6.438394298632498,6.535326967299965,6.632259635967431,6.729192304634898,6.826124973302365,6.31720214102868,6.414134809696147,6.511067478363614,6.60800014703108,6.704932815698547,6.801865484366014,6.898798153033481,6.539716619093227,6.636649287760694,6.733581956428161,6.830514625095628,6.927447293763095,7.024379962430561,7.121312631098028,6.240817004139677,6.337749672807144,6.434682341474611,6.531615010142077,6.628547678809544,6.725480347477011,6.822413016144478,6.063186764974262,6.160119433641729,6.257052102309196,6.353984770976663,6.45091743964413,6.5478501083115965,6.644782776979063,6.280865551163122,6.377798219830589,6.474730888498056,6.571663557165522,6.668596225832989,6.765528894500456,6.862461563167923,6.424747422903067,6.521680091570534,6.618612760238,6.715545428905467,6.812478097572934,6.909410766240401,7.006343434907868,6.3646746023678995,6.4616072710353665,6.5585399397028326,6.6554726083702995,6.7524052770377665,6.8493379457052335,6.9462706143727,6.405846884109144,6.502779552776611,6.5997122214440775,6.6966448901115445,6.7935775587790115,6.8905102274464785,6.9874428961139445,6.005702166879182,6.102634835546649,6.199567504214115,6.296500172881582,6.393432841549049,6.490365510216516,6.587298178883983,6.307190004272819,6.404122672940286,6.501055341607753,6.59798801027522,6.694920678942686,6.791853347610153,6.88878601627762,6.3409383716982894,6.437871040365756,6.534803709033223,6.6317363777006895,6.728669046368156,6.825601715035623,6.92253438370309,6.354662465612038,6.451595134279505,6.548527802946972,6.645460471614438,6.742393140281905,6.839325808949372,6.936258477616839,6.444771696414789,6.541704365082256,6.638637033749723,6.73556970241719,6.832502371084656,6.929435039752123,7.02636770841959,6.424747422903067,6.521680091570534,6.618612760238,6.715545428905467,6.812478097572934,6.909410766240401,7.006343434907868,6.2545410980534255,6.3514737667208925,6.4484064353883594,6.5453391040558255,6.6422717727232925,6.7392044413907595,6.836137110058226,6.472219884242286,6.5691525529097525,6.666085221577219,6.763017890244686,6.8599505589121526,6.9568832275796195,7.0538158962470865,6.58347712327456,6.680409791942027,6.777342460609493,6.87427512927696,6.971207797944427,7.068140466611894,7.165073135279361,6.642426209091926,6.739358877759393,6.83629154642686,6.933224215094327,7.030156883761794,7.12708955242926,7.224022221096727,6.51226843126573,6.609201099933197,6.7061337686006635,6.80306643726813,6.899999105935597,6.996931774603064,7.093864443270531,6.380986918721734,6.477919587389201,6.574852256056668,6.671784924724134,6.768717593391601,6.865650262059068,6.962582930726535,6.28345377360321,6.380386442270677,6.477319110938144,6.574251779605611,6.671184448273077,6.768117116940544,6.865049785608011,6.619813713140117,6.716746381807583,6.81367905047505,6.910611719142517,7.007544387809984,7.104477056477451,7.201409725144918,6.304601781832732,6.401534450500199,6.498467119167666,6.595399787835133,6.692332456502599,6.789265125170066,6.886197793837533,6.184456140762396,6.281388809429863,6.37832147809733,6.475254146764797,6.572186815432264,6.669119484099731,6.766052152767197,6.1058235344377945,6.202756203105261,6.2996888717727275,6.3966215404401945,6.4935542091076615,6.590486877775128,6.687419546442595,6.2545410980534255,6.3514737667208925,6.4484064353883594,6.5453391040558255,6.6422717727232925,6.7392044413907595,6.836137110058226,6.553440713006975,6.650373381674442,6.747306050341909,6.844238719009376,6.941171387676843,7.038104056344309,7.135036725011776,6.579765166116672,6.676697834784139,6.7736305034516056,6.8705631721190725,6.9674958407865395,7.0644285094540065,7.1613611781214725,6.022014483233017,6.118947151900484,6.215879820567951,6.312812489235417,6.409745157902884,6.506677826570351,6.603610495237818,6.136983679423177,6.233916348090644,6.330849016758111,6.427781685425578,6.524714354093045,6.621647022760512,6.718579691427978,6.224504687785842,6.321437356453308,6.418370025120775,6.515302693788242,6.612235362455709,6.709168031123176,6.806100699790642,6.609801576384256,6.706734245051723,6.803666913719189,6.900599582386656,6.997532251054123,7.09446491972159,7.191397588389057,6.3646746023678995,6.4616072710353665,6.5585399397028326,6.6554726083702995,6.7524052770377665,6.8493379457052335,6.9462706143727,6.304601781832732,6.401534450500199,6.498467119167666,6.595399787835133,6.692332456502599,6.789265125170066,6.886197793837533,6.499668072069782,6.596600740737249,6.693533409404716,6.790466078072182,6.887398746739649,6.984331415407116,7.081264084074583,6.258253055211312,6.355185723878779,6.452118392546246,6.549051061213713,6.64598372988118,6.742916398548646,6.839849067216113,6.773707721635923,6.87064039030339,6.967573058970857,7.064505727638323,7.16143839630579,7.258371064973257,7.355303733640724,6.4896559353139205,6.5865886039813875,6.683521272648854,6.780453941316321,6.8773866099837875,6.9743192786512545,7.071251947318721,6.407311371831431,6.504244040498898,6.601176709166365,6.698109377833832,6.795042046501298,6.891974715168765,6.988907383836232,6.354662465612038,6.451595134279505,6.548527802946972,6.645460471614438,6.742393140281905,6.839325808949372,6.936258477616839,6.499668072069782,6.596600740737249,6.693533409404716,6.790466078072182,6.887398746739649,6.984331415407116,7.081264084074583,6.459619525046337,6.556552193713804,6.653484862381271,6.750417531048737,6.847350199716204,6.944282868383671,7.041215537051138,6.539716619093227,6.636649287760694,6.733581956428161,6.830514625095628,6.927447293763095,7.024379962430561,7.121312631098028,6.52486879046168,6.621801459129147,6.718734127796613,6.81566679646408,6.912599465131547,7.009532133799014,7.106464802466481,6.143283859021152,6.240216527688619,6.337149196356085,6.434081865023552,6.531014533691019,6.627947202358486,6.724879871025953,6.390999055477596,6.487931724145063,6.584864392812529,6.681797061479996,6.778729730147463,6.87566239881493,6.972595067482397,6.204480414274119,6.301413082941586,6.398345751609053,6.49527842027652,6.592211088943986,6.689143757611453,6.78607642627892,6.434759559658929,6.531692228326395,6.628624896993862,6.725557565661329,6.822490234328796,6.919422902996263,7.016355571663729,6.593489260030421,6.690421928697887,6.787354597365354,6.884287266032821,6.981219934700288,7.078152603367755,7.175085272035222,6.3821106534395335,6.4790433221070005,6.5759759907744675,6.6729086594419345,6.769841328109401,6.8667739967768675,6.9637066654443345,6.589777302872533,6.68670997154,6.783642640207467,6.880575308874934,6.9775079775424,7.074440646209867,7.171373314877334,6.2545410980534255,6.3514737667208925,6.4484064353883594,6.5453391040558255,6.6422717727232925,6.7392044413907595,6.836137110058226,6.1570079529349,6.253940621602367,6.350873290269834,6.4478059589373,6.544738627604767,6.641671296272234,6.738603964939701,6.553440713006975,6.650373381674442,6.747306050341909,6.844238719009376,6.941171387676843,7.038104056344309,7.135036725011776,6.258253055211312,6.355185723878779,6.452118392546246,6.549051061213713,6.64598372988118,6.742916398548646,6.839849067216113,6.194468277518258,6.291400946185725,6.388333614853192,6.485266283520659,6.582198952188126,6.679131620855592,6.776064289523059,6.793731995147646,6.890664663815112,6.987597332482579,7.084530001150046,7.181462669817513,7.27839533848498,7.375328007152447,6.82376840541523,6.920701074082697,7.017633742750164,7.114566411417631,7.211499080085097,7.308431748752564,7.405364417420031,6.274565371565148,6.3714980402326145,6.468430708900081,6.565363377567548,6.662296046235015,6.759228714902482,6.856161383569949,5.664165782462099,5.761098451129566,5.858031119797033,5.9549637884645,6.051896457131966,6.148829125799433,6.2457617944669,6.234516824541703,6.33144949320917,6.428382161876637,6.5253148305441036,6.6222474992115705,6.719180167879037,6.816112836546504,6.424747422903067,6.521680091570534,6.618612760238,6.715545428905467,6.812478097572934,6.909410766240401,7.006343434907868,6.542304841533314,6.639237510200781,6.736170178868248,6.833102847535715,6.930035516203182,7.026968184870648,7.123900853538115,6.214492551029981,6.311425219697448,6.408357888364915,6.505290557032381,6.602223225699848,6.699155894367315,6.796088563034782,6.063186764974262,6.160119433641729,6.257052102309196,6.353984770976663,6.45091743964413,6.5478501083115965,6.644782776979063,6.234516824541703,6.33144949320917,6.428382161876637,6.5253148305441036,6.6222474992115705,6.719180167879037,6.816112836546504,6.29458964507687,6.391522313744337,6.488454982411804,6.585387651079271,6.682320319746738,6.779252988414204,6.876185657081671,6.257129320493513,6.3540619891609795,6.4509946578284465,6.547927326495913,6.6448599951633796,6.7417926638308465,6.8387253324983135,6.274565371565148,6.3714980402326145,6.468430708900081,6.565363377567548,6.662296046235015,6.759228714902482,6.856161383569949,6.392122790195395,6.489055458862862,6.585988127530329,6.682920796197796,6.779853464865262,6.876786133532729,6.973718802200196,6.434759559658929,6.531692228326395,6.628624896993862,6.725557565661329,6.822490234328796,6.919422902996263,7.016355571663729,6.554905200729263,6.65183786939673,6.748770538064197,6.845703206731664,6.94263587539913,7.039568544066597,7.136501212734064,6.344650328856177,6.441582997523644,6.538515666191111,6.6354483348585775,6.732381003526044,6.829313672193511,6.926246340860978,5.9730775341715105,6.0700102028389775,6.1669428715064445,6.2638755401739115,6.360808208841378,6.4577408775088445,6.5546735461763115,6.683598490833172,6.780531159500639,6.877463828168105,6.974396496835572,7.071329165503039,7.168261834170506,7.265194502837973,6.479643798558059,6.576576467225526,6.673509135892993,6.77044180456046,6.867374473227927,6.964307141895393,7.06123981056286,6.079499081328097,6.176431749995563,6.27336441866303,6.370297087330497,6.467229755997964,6.564162424665431,6.661095093332898,6.347238551296264,6.444171219963731,6.541103888631198,6.638036557298665,6.734969225966132,6.831901894633598,6.928834563301065,6.234516824541703,6.33144949320917,6.428382161876637,6.5253148305441036,6.6222474992115705,6.719180167879037,6.816112836546504,6.150707773336926,6.247640442004392,6.344573110671859,6.441505779339326,6.538438448006793,6.63537111667426,6.732303785341726,6.190756320360371,6.287688989027838,6.384621657695304,6.481554326362771,6.578486995030238,6.675419663697705,6.7723523323651715,6.244528961297564,6.341461629965031,6.438394298632498,6.535326967299965,6.632259635967431,6.729192304634898,6.826124973302365,6.4948323801940955,6.5917650488615624,6.688697717529029,6.785630386196496,6.882563054863963,6.979495723531429,7.076428392198896,6.234516824541703,6.33144949320917,6.428382161876637,6.5253148305441036,6.6222474992115705,6.719180167879037,6.816112836546504,6.099523354839819,6.196456023507286,6.293388692174753,6.39032136084222,6.487254029509687,6.584186698177153,6.68111936684462,6.310901961430705,6.407834630098172,6.504767298765639,6.601699967433106,6.698632636100573,6.79556530476804,6.892497973435506,6.347238551296264,6.444171219963731,6.541103888631198,6.638036557298665,6.734969225966132,6.831901894633598,6.928834563301065,6.643549943809727,6.7404826124771935,6.8374152811446605,6.934347949812127,7.031280618479594,7.128213287147061,7.225145955814528,6.51226843126573,6.609201099933197,6.7061337686006635,6.80306643726813,6.899999105935597,6.996931774603064,7.093864443270531,6.224504687785842,6.321437356453308,6.418370025120775,6.515302693788242,6.612235362455709,6.709168031123176,6.806100699790642,6.444771696414789,6.541704365082256,6.638637033749723,6.73556970241719,6.832502371084656,6.929435039752123,7.02636770841959,6.072075167012323,6.16900783567979,6.265940504347257,6.362873173014723,6.45980584168219,6.556738510349657,6.653671179017124,6.653562080565588,6.750494749233055,6.847427417900522,6.944360086567989,7.041292755235456,7.138225423902923,7.235158092570389,6.554905200729263,6.65183786939673,6.748770538064197,6.845703206731664,6.94263587539913,7.039568544066597,7.136501212734064,6.479643798558059,6.576576467225526,6.673509135892993,6.77044180456046,6.867374473227927,6.964307141895393,7.06123981056286,6.434759559658929,6.531692228326395,6.628624896993862,6.725557565661329,6.822490234328796,6.919422902996263,7.016355571663729,6.394711012635483,6.49164368130295,6.588576349970417,6.685509018637884,6.782441687305351,6.879374355972817,6.976307024640284,6.923889772973842,7.020822441641309,7.117755110308776,7.214687778976243,7.311620447643709,7.408553116311176,7.505485784978643,6.373563004405961,6.470495673073428,6.567428341740895,6.664361010408362,6.761293679075829,6.858226347743296,6.955159016410762,6.2545410980534255,6.3514737667208925,6.4484064353883594,6.5453391040558255,6.6422717727232925,6.7392044413907595,6.836137110058226,6.739959354210452,6.836892022877919,6.933824691545386,7.030757360212853,7.12769002888032,7.224622697547786,7.321555366215253,6.194468277518258,6.291400946185725,6.388333614853192,6.485266283520659,6.582198952188126,6.679131620855592,6.776064289523059,6.35207424317195,6.449006911839417,6.545939580506884,6.642872249174351,6.739804917841817,6.836737586509284,6.933670255176751,6.643549943809727,6.7404826124771935,6.8374152811446605,6.934347949812127,7.031280618479594,7.128213287147061,7.225145955814528,6.324626055344455,6.421558724011922,6.518491392679388,6.615424061346855,6.712356730014322,6.809289398681789,6.906222067349256,6.267141457249374,6.364074125916841,6.461006794584307,6.557939463251774,6.654872131919241,6.751804800586708,6.848737469254175,6.2545410980534255,6.3514737667208925,6.4484064353883594,6.5453391040558255,6.6422717727232925,6.7392044413907595,6.836137110058226,6.479643798558059,6.576576467225526,6.673509135892993,6.77044180456046,6.867374473227927,6.964307141895393,7.06123981056286,6.3646746023678995,6.4616072710353665,6.5585399397028326,6.6554726083702995,6.7524052770377665,6.8493379457052335,6.9462706143727,6.324626055344455,6.421558724011922,6.518491392679388,6.615424061346855,6.712356730014322,6.809289398681789,6.906222067349256,5.965653619855736,6.062586288523203,6.15951895719067,6.256451625858137,6.353384294525604,6.450316963193071,6.547249631860537,6.180744183604509,6.277676852271976,6.374609520939443,6.471542189606909,6.568474858274376,6.665407526941843,6.76234019560931,6.372098516683673,6.469031185351139,6.565963854018606,6.662896522686073,6.75982919135354,6.856761860021007,6.953694528688473,6.07691085888801,6.173843527555476,6.270776196222943,6.36770886489041,6.464641533557877,6.561574202225344,6.658506870892811,6.334638192100315,6.431570860767782,6.528503529435249,6.625436198102716,6.722368866770183,6.81930153543765,6.916234204105116,6.253417363335626,6.350350032003092,6.447282700670559,6.544215369338026,6.641148038005493,6.73808070667296,6.835013375340427,6.475931841400173,6.57286451006764,6.669797178735107,6.766729847402573,6.86366251607004,6.960595184737507,7.057527853404974,6.414735286147206,6.511667954814673,6.60860062348214,6.705533292149606,6.802465960817073,6.89939862948454,6.996331298152007,6.464795969926512,6.561728638593979,6.658661307261445,6.755593975928912,6.852526644596379,6.949459313263846,7.046391981931313,6.689898670431146,6.786831339098613,6.8837640077660796,6.9806966764335465,7.077629345101013,7.17456201376848,7.2714946824359465,6.224504687785842,6.321437356453308,6.418370025120775,6.515302693788242,6.612235362455709,6.709168031123176,6.806100699790642,6.374686739123761,6.471619407791227,6.568552076458694,6.665484745126161,6.762417413793628,6.859350082461095,6.956282751128562,6.422159200462979,6.519091869130445,6.616024537797912,6.712957206465379,6.809889875132846,6.906822543800313,7.00375521246778,6.692486892871233,6.7894195615387,6.886352230206167,6.983284898873634,7.0802175675411005,7.177150236208567,7.274082904876034,6.3821106534395335,6.4790433221070005,6.5759759907744675,6.6729086594419345,6.769841328109401,6.8667739967768675,6.9637066654443345,6.1896325856425705,6.286565254310037,6.383497922977504,6.480430591644971,6.577363260312438,6.674295928979905,6.771228597647371,6.452195610730563,6.54912827939803,6.646060948065497,6.742993616732964,6.83992628540043,6.936858954067897,7.033791622735364,6.354662465612038,6.451595134279505,6.548527802946972,6.645460471614438,6.742393140281905,6.839325808949372,6.936258477616839,6.421035465745179,6.517968134412646,6.614900803080113,6.71183347174758,6.808766140415047,6.905698809082513,7.00263147774998,6.553440713006975,6.650373381674442,6.747306050341909,6.844238719009376,6.941171387676843,7.038104056344309,7.135036725011776,6.434759559658929,6.531692228326395,6.628624896993862,6.725557565661329,6.822490234328796,6.919422902996263,7.016355571663729,6.663574217321449,6.760506885988916,6.857439554656383,6.95437222332385,7.051304891991316,7.148237560658783,7.24517022932625,6.224504687785842,6.321437356453308,6.418370025120775,6.515302693788242,6.612235362455709,6.709168031123176,6.806100699790642,6.234516824541703,6.33144949320917,6.428382161876637,6.5253148305441036,6.6222474992115705,6.719180167879037,6.816112836546504,6.679886533675285,6.776819202342751,6.873751871010218,6.970684539677685,7.067617208345152,7.164549877012619,7.261482545680085,6.204480414274119,6.301413082941586,6.398345751609053,6.49527842027652,6.592211088943986,6.689143757611453,6.78607642627892,6.374686739123761,6.471619407791227,6.568552076458694,6.665484745126161,6.762417413793628,6.859350082461095,6.956282751128562,6.4948323801940955,6.5917650488615624,6.688697717529029,6.785630386196496,6.882563054863963,6.979495723531429,7.076428392198896,6.334638192100315,6.431570860767782,6.528503529435249,6.625436198102716,6.722368866770183,6.81930153543765,6.916234204105116,6.509680208825643,6.60661287749311,6.703545546160576,6.800478214828043,6.89741088349551,6.994343552162977,7.091276220830444,6.348362286014064,6.44529495468153,6.542227623348997,6.639160292016464,6.736092960683931,6.833025629351398,6.929958298018865,6.046874448620426,6.143807117287893,6.24073978595536,6.337672454622827,6.434605123290293,6.53153779195776,6.628470460625227,5.988266115807546,6.085198784475013,6.18213145314248,6.279064121809947,6.375996790477413,6.47292945914488,6.569862127812347,6.482232020998146,6.579164689665613,6.67609735833308,6.773030027000547,6.869962695668014,6.966895364335481,7.063828033002947,6.194468277518258,6.291400946185725,6.388333614853192,6.485266283520659,6.582198952188126,6.679131620855592,6.776064289523059,6.552316978289175,6.649249646956642,6.746182315624109,6.843114984291576,6.940047652959042,7.036980321626509,7.133912990293976,6.2983016022347575,6.395234270902224,6.492166939569691,6.589099608237158,6.6860322769046245,6.7829649455720915,6.8798976142395585,6.220792730627954,6.317725399295421,6.414658067962888,6.511590736630355,6.608523405297822,6.705456073965288,6.802388742632755,6.184456140762396,6.281388809429863,6.37832147809733,6.475254146764797,6.572186815432264,6.669119484099731,6.766052152767197,6.414735286147206,6.511667954814673,6.60860062348214,6.705533292149606,6.802465960817073,6.89939862948454,6.996331298152007,6.619813713140117,6.716746381807583,6.81367905047505,6.910611719142517,7.007544387809984,7.104477056477451,7.201409725144918,5.953053260659789,6.049985929327255,6.146918597994722,6.243851266662189,6.340783935329656,6.4377166039971225,6.5346492726645895,6.036862311864565,6.1337949805320315,6.2307276491994985,6.3276603178669655,6.4245929865344324,6.5215256552018985,6.6184583238693655,6.499668072069782,6.596600740737249,6.693533409404716,6.790466078072182,6.887398746739649,6.984331415407116,7.081264084074583,6.264553234809287,6.361485903476754,6.45841857214422,6.555351240811687,6.652283909479154,6.749216578146621,6.846149246814088,6.312025696148505,6.408958364815972,6.505891033483438,6.602823702150905,6.699756370818372,6.796689039485839,6.893621708153306,6.499668072069782,6.596600740737249,6.693533409404716,6.790466078072182,6.887398746739649,6.984331415407116,7.081264084074583,6.563452849762837,6.660385518430304,6.757318187097771,6.854250855765238,6.951183524432704,7.048116193100171,7.145048861767638,6.484820243438234,6.581752912105701,6.678685580773168,6.775618249440635,6.872550918108102,6.969483586775569,7.066416255443035,6.603501396786282,6.700434065453749,6.797366734121216,6.894299402788683,6.9912320714561496,7.0881647401236165,7.185097408791083,6.4696316618021985,6.566564330469665,6.663496999137132,6.7604296678045985,6.8573623364720655,6.9542950051395325,7.0512276738069986,6.204480414274119,6.301413082941586,6.398345751609053,6.49527842027652,6.592211088943986,6.689143757611453,6.78607642627892,6.548605021131289,6.645537689798756,6.742470358466222,6.839403027133689,6.936335695801156,7.033268364468623,7.13020103313609,6.204480414274119,6.301413082941586,6.398345751609053,6.49527842027652,6.592211088943986,6.689143757611453,6.78607642627892,6.244528961297564,6.341461629965031,6.438394298632498,6.535326967299965,6.632259635967431,6.729192304634898,6.826124973302365,6.593489260030421,6.690421928697887,6.787354597365354,6.884287266032821,6.981219934700288,7.078152603367755,7.175085272035222,6.519692345581505,6.616625014248971,6.713557682916438,6.810490351583905,6.907423020251372,7.004355688918839,7.101288357586306,6.549728755849088,6.646661424516555,6.743594093184022,6.840526761851489,6.937459430518955,7.034392099186422,7.131324767853889,6.553440713006975,6.650373381674442,6.747306050341909,6.844238719009376,6.941171387676843,7.038104056344309,7.135036725011776,6.699910807187007,6.796843475854474,6.893776144521941,6.990708813189407,7.087641481856874,7.184574150524341,7.281506819191808,6.3821106534395335,6.4790433221070005,6.5759759907744675,6.6729086594419345,6.769841328109401,6.8667739967768675,6.9637066654443345,6.39842296979337,6.495355638460836,6.592288307128303,6.68922097579577,6.786153644463237,6.883086313130704,6.980018981798171,6.573464986518698,6.670397655186165,6.767330323853632,6.864262992521098,6.961195661188565,7.058128329856032,7.155060998523499,6.342062106416089,6.438994775083556,6.535927443751023,6.63286011241849,6.729792781085957,6.826725449753423,6.92365811842089,6.544893063973402,6.641825732640869,6.738758401308336,6.835691069975803,6.93262373864327,7.029556407310736,7.126489075978203,6.264553234809287,6.361485903476754,6.45841857214422,6.555351240811687,6.652283909479154,6.749216578146621,6.846149246814088,6.224504687785842,6.321437356453308,6.418370025120775,6.515302693788242,6.612235362455709,6.709168031123176,6.806100699790642,6.529704482337365,6.626637151004832,6.723569819672299,6.820502488339766,6.917435157007233,7.0143678256747,7.111300494342166,6.214492551029981,6.311425219697448,6.408357888364915,6.505290557032381,6.602223225699848,6.699155894367315,6.796088563034782,6.3821106534395335,6.4790433221070005,6.5759759907744675,6.6729086594419345,6.769841328109401,6.8667739967768675,6.9637066654443345,6.479643798558059,6.576576467225526,6.673509135892993,6.77044180456046,6.867374473227927,6.964307141895393,7.06123981056286,6.58347712327456,6.680409791942027,6.777342460609493,6.87427512927696,6.971207797944427,7.068140466611894,7.165073135279361,6.459619525046337,6.556552193713804,6.653484862381271,6.750417531048737,6.847350199716204,6.944282868383671,7.041215537051138,6.43217133721884,6.529104005886307,6.626036674553774,6.722969343221241,6.819902011888708,6.916834680556175,7.013767349223641,6.6461381662498145,6.743070834917281,6.8400035035847475,6.9369361722522145,7.0338688409196815,7.130801509587148,7.227734178254615,6.479643798558059,6.576576467225526,6.673509135892993,6.77044180456046,6.867374473227927,6.964307141895393,7.06123981056286,6.314613918588593,6.41154658725606,6.508479255923527,6.605411924590993,6.70234459325846,6.799277261925927,6.896209930593394,6.29458964507687,6.391522313744337,6.488454982411804,6.585387651079271,6.682320319746738,6.779252988414204,6.876185657081671,6.80003217474562,6.896964843413087,6.9938975120805535,7.0908301807480205,7.187762849415487,7.284695518082954,7.3816281867504205,6.464795969926512,6.561728638593979,6.658661307261445,6.755593975928912,6.852526644596379,6.949459313263846,7.046391981931313,6.224504687785842,6.321437356453308,6.418370025120775,6.515302693788242,6.612235362455709,6.709168031123176,6.806100699790642,6.543428576251115,6.640361244918581,6.737293913586048,6.834226582253515,6.931159250920982,7.028091919588449,7.125024588255915,6.6023776620684815,6.6993103307359485,6.7962429994034155,6.893175668070882,6.9901083367383485,7.0870410054058155,7.1839736740732825,6.362086379927812,6.4590190485952785,6.5559517172627455,6.652884385930212,6.749817054597679,6.8467497232651455,6.9436823919326125,6.330926234942428,6.427858903609895,6.524791572277362,6.621724240944829,6.718656909612295,6.815589578279762,6.912522246947229,6.4696316618021985,6.566564330469665,6.663496999137132,6.7604296678045985,6.8573623364720655,6.9542950051395325,7.0512276738069986,6.407311371831431,6.504244040498898,6.601176709166365,6.698109377833832,6.795042046501298,6.891974715168765,6.988907383836232,6.372098516683673,6.469031185351139,6.565963854018606,6.662896522686073,6.75982919135354,6.856761860021007,6.953694528688473,6.344650328856177,6.441582997523644,6.538515666191111,6.6354483348585775,6.732381003526044,6.829313672193511,6.926246340860978,6.234516824541703,6.33144949320917,6.428382161876637,6.5253148305441036,6.6222474992115705,6.719180167879037,6.816112836546504,6.234516824541703,6.33144949320917,6.428382161876637,6.5253148305441036,6.6222474992115705,6.719180167879037,6.816112836546504,6.11065922631348,6.207591894980947,6.304524563648414,6.401457232315881,6.498389900983348,6.595322569650815,6.692255238318281,6.204480414274119,6.301413082941586,6.398345751609053,6.49527842027652,6.592211088943986,6.689143757611453,6.78607642627892,6.194468277518258,6.291400946185725,6.388333614853192,6.485266283520659,6.582198952188126,6.679131620855592,6.776064289523059,6.362086379927812,6.4590190485952785,6.5559517172627455,6.652884385930212,6.749817054597679,6.8467497232651455,6.9436823919326125,6.174444004006536,6.271376672674002,6.368309341341469,6.465242010008936,6.562174678676403,6.65910734734387,6.756040016011337,6.2545410980534255,6.3514737667208925,6.4484064353883594,6.5453391040558255,6.6422717727232925,6.7392044413907595,6.836137110058226,6.288289465478896,6.385222134146363,6.482154802813829,6.579087471481296,6.676020140148763,6.77295280881623,6.869885477483697,6.479643798558059,6.576576467225526,6.673509135892993,6.77044180456046,6.867374473227927,6.964307141895393,7.06123981056286,6.204480414274119,6.301413082941586,6.398345751609053,6.49527842027652,6.592211088943986,6.689143757611453,6.78607642627892,6.603501396786282,6.700434065453749,6.797366734121216,6.894299402788683,6.9912320714561496,7.0881647401236165,7.185097408791083,6.174444004006536,6.271376672674002,6.368309341341469,6.465242010008936,6.562174678676403,6.65910734734387,6.756040016011337,6.224504687785842,6.321437356453308,6.418370025120775,6.515302693788242,6.612235362455709,6.709168031123176,6.806100699790642,6.204480414274119,6.301413082941586,6.398345751609053,6.49527842027652,6.592211088943986,6.689143757611453,6.78607642627892,6.499668072069782,6.596600740737249,6.693533409404716,6.790466078072182,6.887398746739649,6.984331415407116,7.081264084074583,6.146995816179039,6.2439284848465055,6.3408611535139725,6.4377938221814395,6.5347264908489056,6.6316591595163725,6.7285918281838395,6.274565371565148,6.3714980402326145,6.468430708900081,6.565363377567548,6.662296046235015,6.759228714902482,6.856161383569949,6.459619525046337,6.556552193713804,6.653484862381271,6.750417531048737,6.847350199716204,6.944282868383671,7.041215537051138,6.464795969926512,6.561728638593979,6.658661307261445,6.755593975928912,6.852526644596379,6.949459313263846,7.046391981931313,6.184456140762396,6.281388809429863,6.37832147809733,6.475254146764797,6.572186815432264,6.669119484099731,6.766052152767197,6.199644722398432,6.296577391065899,6.393510059733366,6.490442728400833,6.5873753970683,6.684308065735766,6.781240734403233,6.288289465478896,6.385222134146363,6.482154802813829,6.579087471481296,6.676020140148763,6.77295280881623,6.869885477483697,5.959353440257762,6.056286108925229,6.153218777592696,6.250151446260162,6.347084114927629,6.444016783595096,6.540949452262563,6.224504687785842,6.321437356453308,6.418370025120775,6.515302693788242,6.612235362455709,6.709168031123176,6.806100699790642,6.164431867250674,6.261364535918141,6.358297204585607,6.455229873253074,6.552162541920541,6.649095210588008,6.746027879255475],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Age\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Wage\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Wage vs. Age\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ade5a59c-42f6-425f-881b-ad18c4bb125e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#si-linear-regression\n",
        "\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import plotly.express as px\n",
        "\n",
        "\n",
        "url = \"https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/wagePanelData.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "\n",
        "# Assuming 'exp' is the actual name for 'experience', and 'age' is indeed present:\n",
        "X = data[['year', 'education', 'years_experience']]  # Changed 'experience' to 'exp'\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "y = data['log_wage']\n",
        "reg = sm.OLS(y, X).fit()\n",
        "\n",
        "print(reg.summary())\n",
        "fig = px.scatter(data, x='year', y='log_wage', title='Wage vs. Age', labels={'year': 'Age', 'log_wage': 'Wage'})\n",
        "fig.add_scatter(x=data['year'], y=reg.fittedvalues, mode='lines', name='Fitted Line', line=dict(color='red'))\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYW9haZKuq5d"
      },
      "source": [
        "## Solve-it!\n",
        "\n",
        "Import the pass/fail data for students in Portugal found here(https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/passFailTrain.csv), and create a logistic regression model\n",
        "using `statsmodels` that can estimate the likelihood of students passing or failing class. The dependent variable is contained in the column called `G3`, which takes the value `1` when the student has a passing final grade, and `0` otherwise.\n",
        "\n",
        "Call your fitted model `reg`, and place all code for this exercise in the cell labeled `#si-logistic-regression` file found below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVi1Nilnuq5d",
        "outputId": "6ec69351-847b-4391-ef00-c9cfb6376330"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  school  sex  age  address  famsize  Pstatus  Medu  Fedu  Mjob  \\\n",
            "0          16       0    0   16        1        0        1     4     4     3   \n",
            "1          66       0    1   15        1        0        0     4     4     2   \n",
            "2         211       0    1   17        1        1        1     4     4     3   \n",
            "3           7       0    0   17        1        0        0     4     4     2   \n",
            "4          19       0    1   16        1        1        1     4     3     1   \n",
            "\n",
            "   ...  famrel  freetime  goout  Dalc  Walc  health  absences  G1  G2  G3  \n",
            "0  ...       3         2      3     1     2       2         6   1   1   1  \n",
            "1  ...       1         3      3     5     5       3         4   1   1   1  \n",
            "2  ...       5         3      5     4     5       3        13   1   1   1  \n",
            "3  ...       4         1      4     1     1       1         6   0   1   0  \n",
            "4  ...       3         1      3     1     3       5         4   0   1   1  \n",
            "\n",
            "[5 rows x 34 columns]\n",
            "       Unnamed: 0      school         sex         age     address     famsize  \\\n",
            "count  296.000000  296.000000  296.000000  296.000000  296.000000  296.000000   \n",
            "mean   199.540541    0.101351    0.462838   16.736486    0.790541    0.290541   \n",
            "std    110.973191    0.302304    0.499461    1.274849    0.407612    0.454781   \n",
            "min      1.000000    0.000000    0.000000   15.000000    0.000000    0.000000   \n",
            "25%    105.750000    0.000000    0.000000   16.000000    1.000000    0.000000   \n",
            "50%    204.500000    0.000000    0.000000   17.000000    1.000000    0.000000   \n",
            "75%    293.250000    0.000000    1.000000   18.000000    1.000000    1.000000   \n",
            "max    394.000000    1.000000    1.000000   22.000000    1.000000    1.000000   \n",
            "\n",
            "          Pstatus        Medu        Fedu        Mjob  ...      famrel  \\\n",
            "count  296.000000  296.000000  296.000000  296.000000  ...  296.000000   \n",
            "mean     0.898649    2.746622    2.540541    2.141892  ...    3.925676   \n",
            "std      0.302304    1.079857    1.075952    1.232389  ...    0.887512   \n",
            "min      0.000000    0.000000    0.000000    0.000000  ...    1.000000   \n",
            "25%      1.000000    2.000000    2.000000    2.000000  ...    4.000000   \n",
            "50%      1.000000    3.000000    2.000000    2.000000  ...    4.000000   \n",
            "75%      1.000000    4.000000    3.000000    3.000000  ...    5.000000   \n",
            "max      1.000000    4.000000    4.000000    4.000000  ...    5.000000   \n",
            "\n",
            "         freetime       goout        Dalc        Walc      health    absences  \\\n",
            "count  296.000000  296.000000  296.000000  296.000000  296.000000  296.000000   \n",
            "mean     3.236486    3.175676    1.516892    2.361486    3.523649    6.016892   \n",
            "std      1.030790    1.127266    0.913024    1.312677    1.418802    8.541606   \n",
            "min      1.000000    1.000000    1.000000    1.000000    1.000000    0.000000   \n",
            "25%      3.000000    2.000000    1.000000    1.000000    3.000000    0.000000   \n",
            "50%      3.000000    3.000000    1.000000    2.000000    4.000000    4.000000   \n",
            "75%      4.000000    4.000000    2.000000    3.000000    5.000000    8.000000   \n",
            "max      5.000000    5.000000    5.000000    5.000000    5.000000   75.000000   \n",
            "\n",
            "               G1          G2          G3  \n",
            "count  296.000000  296.000000  296.000000  \n",
            "mean     0.665541    0.962838    0.682432  \n",
            "std      0.472600    0.189479    0.466319  \n",
            "min      0.000000    0.000000    0.000000  \n",
            "25%      0.000000    1.000000    0.000000  \n",
            "50%      1.000000    1.000000    1.000000  \n",
            "75%      1.000000    1.000000    1.000000  \n",
            "max      1.000000    1.000000    1.000000  \n",
            "\n",
            "[8 rows x 34 columns]\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.313771\n",
            "         Iterations 23\n",
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:                     G3   No. Observations:                  296\n",
            "Model:                          Logit   Df Residuals:                      262\n",
            "Method:                           MLE   Df Model:                           33\n",
            "Date:                Fri, 15 Nov 2024   Pseudo R-squ.:                  0.4980\n",
            "Time:                        18:40:36   Log-Likelihood:                -92.876\n",
            "converged:                       True   LL-Null:                       -185.01\n",
            "Covariance Type:            nonrobust   LLR p-value:                 6.314e-23\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const        -37.8410   2.24e+14  -1.69e-13      1.000   -4.38e+14    4.38e+14\n",
            "Unnamed: 0     0.0002      0.003      0.058      0.953      -0.007       0.007\n",
            "school        -0.2226      0.794     -0.280      0.779      -1.779       1.333\n",
            "sex            0.4512      0.536      0.842      0.400      -0.599       1.501\n",
            "age           -0.6683      0.290     -2.305      0.021      -1.236      -0.100\n",
            "address       -0.2726      0.521     -0.524      0.600      -1.293       0.748\n",
            "famsize       -0.0655      0.457     -0.143      0.886      -0.962       0.831\n",
            "Pstatus       -0.6200      0.679     -0.913      0.361      -1.951       0.711\n",
            "Medu           0.1185        nan        nan        nan         nan         nan\n",
            "Fedu          -0.1614      0.115     -1.404      0.160      -0.387       0.064\n",
            "Mjob          -0.2220      0.140     -1.589      0.112      -0.496       0.052\n",
            "Fjob           0.0388      0.254      0.153      0.879      -0.460       0.537\n",
            "reason         0.1785      0.178      1.001      0.317      -0.171       0.528\n",
            "guardian       0.3266      0.372      0.878      0.380      -0.403       1.056\n",
            "traveltime     0.2017      0.301      0.671      0.502      -0.388       0.791\n",
            "studytime     -0.1899      0.328     -0.580      0.562      -0.832       0.452\n",
            "failures      -0.4699      0.283     -1.662      0.096      -1.024       0.084\n",
            "schoolsup     -0.4917      0.573     -0.858      0.391      -1.615       0.632\n",
            "famsup        -1.0076      0.455     -2.212      0.027      -1.900      -0.115\n",
            "paid           0.7327      0.456      1.606      0.108      -0.162       1.627\n",
            "activities    -0.3223      0.437     -0.738      0.461      -1.179       0.534\n",
            "nursery       -0.4218      0.541     -0.780      0.435      -1.482       0.638\n",
            "higher         1.4193      0.953      1.489      0.136      -0.449       3.288\n",
            "internet      -0.0501      0.465     -0.108      0.914      -0.962       0.862\n",
            "romantic      -0.5576      0.432     -1.291      0.197      -1.404       0.289\n",
            "famrel         0.3461      0.131      2.638      0.008       0.089       0.603\n",
            "freetime      -0.0304      0.211     -0.144      0.885      -0.443       0.383\n",
            "goout         -0.3697      0.210     -1.763      0.078      -0.781       0.041\n",
            "Dalc          -0.2911      0.276     -1.054      0.292      -0.832       0.250\n",
            "Walc           0.1672      0.243      0.689      0.491      -0.308       0.643\n",
            "health        -0.0620      0.152     -0.408      0.684      -0.360       0.236\n",
            "absences      -0.0433      0.022     -1.972      0.049      -0.086      -0.000\n",
            "G1             3.6013      0.492      7.320      0.000       2.637       4.566\n",
            "G2            49.1062   2.24e+14    2.2e-13      1.000   -4.38e+14    4.38e+14\n",
            "==============================================================================\n"
          ]
        }
      ],
      "source": [
        "#si-logistic-regression\n",
        "\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Step 1: Load the dataset from the provided URL\n",
        "url = \"https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/passFailTrain.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Explore the data to understand its structure (for checking purposes)\n",
        "print(data.head())\n",
        "print(data.describe())\n",
        "\n",
        "# Step 3: Prepare the data\n",
        "# The dependent variable is G3, where 1 indicates passing and 0 indicates failing.\n",
        "# Independent variables will be the remaining columns (we can use all or a subset of them).\n",
        "\n",
        "# Dependent variable: G3 (passing/failing status)\n",
        "y = data['G3']\n",
        "\n",
        "# Independent variables: Select all columns except 'G3'\n",
        "X = data.drop(columns=['G3'])\n",
        "\n",
        "# Add a constant term (intercept) to the model\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Step 4: Fit the logistic regression model using statsmodels\n",
        "logit_model = sm.Logit(y, X)\n",
        "reg = logit_model.fit()\n",
        "\n",
        "# Step 5: Print the summary of the regression results\n",
        "print(reg.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bF6COwmVuq5d"
      },
      "source": [
        "## Solve-it!\n",
        "\n",
        "Use the data on NFL franchise values included in the NFL Valuation data source (https://raw.githubusercontent.com/dustywhite7/Econ8320/master/AssignmentData/assignment12Data.csv) file to implement a Random Forest Classifier in sklearn using 100 trees to predict team-years when `Playoffs` takes the value `1` (when a team made the playoffs in that season).\n",
        "\n",
        "- Use Patsy to create `x2` and `y2` matrices\n",
        "- Create the classifier\n",
        "- Fit the classifier, and store the fitted model with the name `playoffForest`\n",
        "\n",
        "Place all code for this exercise in the cell labeled `#si-random-forest` file found below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "gYlwe_uYuq5d",
        "outputId": "4246dde1-414b-4476-d2cf-aa3e53ace5d3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<unknown>, line 1)",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \u001b[1;32m\"<ipython-input-42-83ae0855e915>\"\u001b[0m, line \u001b[1;32m11\u001b[0m, in \u001b[1;35m<cell line: 11>\u001b[0m\n    y2, x2 = pt.dmatrices(\"Playoffs ~ .\", data=data)\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/patsy/highlevel.py\"\u001b[0m, line \u001b[1;32m309\u001b[0m, in \u001b[1;35mdmatrices\u001b[0m\n    (lhs, rhs) = _do_highlevel_design(formula_like, data, eval_env,\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/patsy/highlevel.py\"\u001b[0m, line \u001b[1;32m164\u001b[0m, in \u001b[1;35m_do_highlevel_design\u001b[0m\n    design_infos = _try_incr_builders(formula_like, data_iter_maker, eval_env,\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/patsy/highlevel.py\"\u001b[0m, line \u001b[1;32m66\u001b[0m, in \u001b[1;35m_try_incr_builders\u001b[0m\n    return design_matrix_builders([formula_like.lhs_termlist,\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/patsy/build.py\"\u001b[0m, line \u001b[1;32m689\u001b[0m, in \u001b[1;35mdesign_matrix_builders\u001b[0m\n    factor_states = _factors_memorize(all_factors, data_iter_maker, eval_env)\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/patsy/build.py\"\u001b[0m, line \u001b[1;32m354\u001b[0m, in \u001b[1;35m_factors_memorize\u001b[0m\n    which_pass = factor.memorize_passes_needed(state, eval_env)\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/patsy/eval.py\"\u001b[0m, line \u001b[1;32m478\u001b[0m, in \u001b[1;35mmemorize_passes_needed\u001b[0m\n    subset_names = [name for name in ast_names(self.code)\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/patsy/eval.py\"\u001b[0m, line \u001b[1;32m478\u001b[0m, in \u001b[1;35m<listcomp>\u001b[0m\n    subset_names = [name for name in ast_names(self.code)\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/patsy/eval.py\"\u001b[0m, line \u001b[1;32m109\u001b[0m, in \u001b[1;35mast_names\u001b[0m\n    for node in ast.walk(ast.parse(code)):\n",
            "\u001b[0;36m  File \u001b[0;32m\"/usr/lib/python3.10/ast.py\"\u001b[0;36m, line \u001b[0;32m50\u001b[0;36m, in \u001b[0;35mparse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, mode, flags,\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    .\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#si-random-forest\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import patsy as pt\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/dustywhite7/Econ8320/master/AssignmentData/assignment12Data.csv\")\n",
        "\n",
        "# Create Patsy matrices\n",
        "y2, x2 = pt.dmatrices(\"Playoffs ~ .\", data=data)\n",
        "\n",
        "# Create and fit the classifier\n",
        "playoffForest = RandomForestClassifier(n_estimators=100)\n",
        "playoffForest.fit(x2, y2.ravel())"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}